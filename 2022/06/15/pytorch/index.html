<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>pytorch | jjyaoao's Home</title><meta name="keywords" content="pytorch"><meta name="author" content="jjyaoao,jjyaoao@126.com"><meta name="copyright" content="jjyaoao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="PyTorchYYDS">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch">
<meta property="og:url" content="http://jjyaoao.space/2022/06/15/pytorch/index.html">
<meta property="og:site_name" content="jjyaoao&#39;s Home">
<meta property="og:description" content="PyTorchYYDS">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2022/04/15/YXoFfbxgWZIn1SM.png">
<meta property="article:published_time" content="2022-06-15T04:34:21.000Z">
<meta property="article:modified_time" content="2022-10-29T14:47:35.297Z">
<meta property="article:author" content="jjyaoao">
<meta property="article:tag" content="遗忘指南">
<meta property="article:tag" content="学习笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2022/04/15/YXoFfbxgWZIn1SM.png"><link rel="shortcut icon" href="https://s2.loli.net/2022/01/09/le6jcmPKuJihDdU.jpg"><link rel="canonical" href="http://jjyaoao.space/2022/06/15/pytorch/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?06fb7a80772d98cbf9e7beeb76e262e4";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JOZPQM4N5F","apiKey":"42451c032f2166ed9800520810e2d223","indexName":"jjyaoao-blog","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-10-29 22:47:35'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.0.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2022/01/09/le6jcmPKuJihDdU.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">87</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音樂</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2022/04/15/YXoFfbxgWZIn1SM.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">jjyaoao's Home</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音樂</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-06-15T04:34:21.000Z" title="发表于 2022-06-15 12:34:21">2022-06-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-10-29T14:47:35.297Z" title="更新于 2022-10-29 22:47:35">2022-10-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/pytorch/">pytorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">17.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>69分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="pytorch"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><p>打开Anaconda Prompt (Anaconda3)</p>
<p><strong>创建环境</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">*<span class="comment">## conda 是指调用 conda 包，create 是创建的意思* </span></span><br><span class="line">*<span class="comment">## -n 是指后面的名字是屋子的名字* </span></span><br><span class="line">*<span class="comment">## pytorch是屋子的名字（可以更改成自己喜欢的）* </span></span><br><span class="line">*<span class="comment">## python=3.6 是指创建的屋子，是 python3.6 版本。* </span></span><br><span class="line">conda create -n pytorch python=3.6  </span><br></pre></td></tr></table></figure>

<p>输入<code>conda activate pytorch</code>进入pytorch操作界面</p>
<p><code>pip install xxx(==***)</code>下载相关包，==后是版本号，可忽略</p>
<p><code>pip uninstall xxx</code> 删除相关包</p>
<p><code>conda info --envs</code>查看已经创建好的房间</p>
<p><code>conda activate pytorch</code>激活python房间</p>
<p>腾讯云配置pytorch</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/150632191">Pytorch入门——手把手带你配置云服务器环境 - 知乎 (zhihu.com)</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ipynb转markdown</span></span><br><span class="line"> jupyter nbconvert --to markdown *.ipynb</span><br><span class="line"> </span><br><span class="line"><span class="comment"># ipynb转py</span></span><br><span class="line">jupyter nbconvert --to script file_name.ipynb</span><br><span class="line"></span><br><span class="line"><span class="comment"># py转ipynb</span></span><br><span class="line">1.%load语句，在notebook新建一个ipynb记事本输入%load file_name.py就会把整个py文件的代码加载到一个cell里面来，之后自己整理了。</span><br><span class="line">2.如果只是单纯要调用py文件的函数的话可以像一般掉包一样from xx import xx但是脚本语言嘛，平时写的py文件可能只是处理一些临时事务，就没有封装好，可能都没有main函数，是从头到尾顺序执行，这个时候就要用到%run语句了，相当于直接执行py文件。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="一点点学习路线"><a href="#一点点学习路线" class="headerlink" title="一点点学习路线"></a>一点点学习路线</h1><p>朴素贝叶斯，逻辑回归最大熵 ，提升树看gdbt和adboost？xgboost，必须要手推公式</p>
<p>有不会的去看刘建平老师的博客</p>
<p><img src="/2022/06/15/pytorch/image-20220629155332703.png" alt="image-20220629155332703"></p>
<p><img src="/2022/06/15/pytorch/image-20220629155416050.png" alt="image-20220629155416050"></p>
<p><img src="/2022/06/15/pytorch/image-20220629155450487.png" alt="image-20220629155450487"></p>
<p>机器学习实战入门看两个任务</p>
<h1 id="学习实践"><a href="#学习实践" class="headerlink" title="学习实践"></a>学习实践</h1><blockquote>
<p>本实践全部来自于课程<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Y7411d7Ys?p=2&vd_source=55ffe5143706c86851979f75f0da6acb">《PyTorch深度学习实践》完结合集_哔哩哔哩_bilibili</a></p>
</blockquote>
<h2 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h2><p>$$<br>\hat{y}=x*w<br>$$</p>
<p>解释数据集和模型，这里我们先完成不加b的情况</p>
<p><img src="/2022/06/15/pytorch/image-20220629120239638.png" alt="image-20220629120239638"></p>
<p>取MSE为损失函数</p>
<p><img src="/2022/06/15/pytorch/image-20220629120944968.png" alt="image-20220629120944968"></p>
<p>开始构建模型，手撸冲冲冲</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_data = [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]</span><br><span class="line">y_data = [<span class="number">2.</span>, <span class="number">4.</span>, <span class="number">6.</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x_data</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x_data * w</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">y_pred, y_data</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (y_pred - y_data) ** <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">w_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    loss_ = <span class="number">0</span> <span class="comment"># 注意别放到for外面了，每轮都要刷新一次loss，算当前w的loss</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):    </span><br><span class="line">        y_pred = forward(x_val)</span><br><span class="line">        loss_ += loss(y_pred, y_val)</span><br><span class="line">    loss_ /= <span class="built_in">len</span>(x_data)</span><br><span class="line">    w_list.append(w)  </span><br><span class="line">    mse_list.append(loss_)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">7</span>,<span class="number">3</span>), dpi=<span class="number">120</span>)</span><br><span class="line">plt.plot(w_list, mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="/2022/06/15/pytorch/output-16565071400328.png" alt="output"></p>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><blockquote>
<p>引言</p>
</blockquote>
<p>​        让我们从导数开始讲起，一个点在该处对某一个方向的导数，是梯度，我们也可以看做是方向，但是这个方向，比如这里，此时他的导数是正的，但是很显然，我们应该往与他导数相反的方向前进，才能得到我们想要的结果，也就是min(cost)，因此我们的迭代函数为下图中的update，其中α为学习率</p>
<ul>
<li>当α比较大时，可能会发生跨度太大，从而一步就错过了最低点，然后就不断的交叉交叉，始终无法逼近最小点</li>
<li>当α太小时，可能会陷入局部最优走不出来，这也是很不好的，不过我们还是应该适当取得小一点，因为后续有方法可以尽可能客服局部最优的问题</li>
</ul>
<p>​        通过我们这种一小步一小步逼近的方法，其实就反应了DSA中的<code>贪心思想</code></p>
<p><img src="/2022/06/15/pytorch/image-20220629183852779.png" alt="image-20220629183852779"></p>
<p>这个是梯度的更新公式：我们推导出以后直接定义graident函数就好</p>
<p><img src="/2022/06/15/pytorch/image-20220629183749870.png" alt="image-20220629183749870"></p>
<p>以下是实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">	<span class="keyword">return</span> x * w</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">xs, ys</span>):</span></span><br><span class="line">	cost = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        cost += (y_pred - y) ** <span class="number">2</span></span><br><span class="line">	<span class="keyword">return</span> cost / <span class="built_in">len</span>(xs)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">xs, ys</span>):</span></span><br><span class="line">	grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">    	grad += <span class="number">2</span> * x * (x * w - y)</span><br><span class="line">	<span class="keyword">return</span> grad / <span class="built_in">len</span>(xs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">	cost_val = cost(x_data, y_data)</span><br><span class="line">	grad_val = gradient(x_data, y_data)</span><br><span class="line">	w -= <span class="number">0.01</span> * grad_val</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, cost_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>

<p>版本二（自己实现）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x*w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> ((forward(x)-y) ** <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*x*(x*w-y)</span><br><span class="line">epoch_list = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">101</span>)]</span><br><span class="line">cost_list = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">101</span>):</span><br><span class="line">    cost_ = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_digit, y_digit <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        cost_ =+ cost(x_digit, y_digit)  </span><br><span class="line">        w -= <span class="number">0.01</span>*gradient(x_digit, y_digit) </span><br><span class="line">    cost_ /= <span class="built_in">len</span>(x_data)</span><br><span class="line">    cost_list.append(cost_)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;cost: &#x27;</span>, cost_, <span class="string">&#x27;w: &#x27;</span>, w)</span><br><span class="line">plt.figure(figsize=(<span class="number">7</span>,<span class="number">3</span>), dpi=<span class="number">120</span>)</span><br><span class="line">plt.plot(epoch_list, cost_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;cost&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">cost:  <span class="number">2.4386476799999994</span> w:  <span class="number">1.260688</span></span><br><span class="line">cost:  <span class="number">1.3329214952735635</span> w:  <span class="number">1.453417766656</span></span><br><span class="line">cost:  <span class="number">0.7285512077588492</span> w:  <span class="number">1.5959051959019805</span></span><br><span class="line">cost:  <span class="number">0.3982131462423004</span> w:  <span class="number">1.701247862192685</span></span><br><span class="line">cost:  <span class="number">0.21765623082005736</span> w:  <span class="number">1.7791289594933983</span></span><br><span class="line">cost:  <span class="number">0.11896702874286423</span> w:  <span class="number">1.836707389300983</span></span><br><span class="line">cost:  <span class="number">0.06502526426457467</span> w:  <span class="number">1.8792758133988885</span></span><br><span class="line">cost:  <span class="number">0.03554165416551504</span> w:  <span class="number">1.910747160155559</span></span><br><span class="line">cost:  <span class="number">0.019426436710527316</span> w:  <span class="number">1.9340143044689266</span></span><br><span class="line">cost:  <span class="number">0.010618145163155871</span> w:  <span class="number">1.9512159834655312</span></span><br><span class="line">cost:  <span class="number">0.005803689497248529</span> w:  <span class="number">1.9639333911678687</span></span><br><span class="line">cost:  <span class="number">0.0031721935670412514</span> w:  <span class="number">1.9733355232910992</span></span><br><span class="line">cost:  <span class="number">0.0017338646444728853</span> w:  <span class="number">1.9802866323953892</span></span><br><span class="line">cost:  <span class="number">0.0009476996096921988</span> w:  <span class="number">1.9854256707695</span></span><br><span class="line">cost:  <span class="number">0.0005179957692047892</span> w:  <span class="number">1.9892250235079405</span></span><br><span class="line">cost:  <span class="number">0.00028312728439470265</span> w:  <span class="number">1.9920339305797026</span></span><br><span class="line">cost:  <span class="number">0.0001547523434250841</span> w:  <span class="number">1.994110589284741</span></span><br><span class="line">cost:  <span class="number">8.458488148449721e-05</span> w:  <span class="number">1.9956458879852805</span></span><br><span class="line">cost:  <span class="number">4.6232593428939405e-05</span> w:  <span class="number">1.9967809527381737</span></span><br><span class="line">cost:  <span class="number">2.526991416967284e-05</span> w:  <span class="number">1.9976201197307648</span></span><br><span class="line">...</span><br><span class="line">cost:  <span class="number">8.693904559623234e-26</span> w:  <span class="number">1.9999999999998603</span></span><br><span class="line">cost:  <span class="number">4.7496000335181754e-26</span> w:  <span class="number">1.9999999999998967</span></span><br><span class="line">cost:  <span class="number">2.6091574440184965e-26</span> w:  <span class="number">1.9999999999999236</span></span><br><span class="line">cost:  <span class="number">1.4153216454205246e-26</span> w:  <span class="number">1.9999999999999436</span></span><br></pre></td></tr></table></figure>

<p><img src="/2022/06/15/pytorch/output-16565071114086.png" alt="output"></p>
<p>​        </p>
<p>​        如果曲线不是这么的平滑，或者局部震荡很大，我们通常可以采用指数加权均值，举个例子，设我们现在每一步的loss为$c\begin{array}{c}0\\end{array},c\begin{array}{c}1\\end{array},c\begin{array}{c}2\\end{array}….$</p>
<p>设指数加权均值后loss为$c\begin{array}{c} ‘\ 0\\end{array},c\begin{array}{c}’\ 1\\end{array},c\begin{array}{c}  ‘\  2\\end{array}….$</p>
<p>则现在可以得到该转换公式$\left{ \begin{array}{l}  c\begin{array}{c}  ‘\  0\\end{array}=c\begin{array}{c}  \  0\\end{array}\  c\begin{array}{c}  ‘\  i\\end{array}=\beta c\begin{array}{c}  \  i\\end{array}+\left( 1-\beta \right) c\begin{array}{c}  ‘\  i-1\\end{array}\  i\ne 0\\end{array} \right. $</p>
<p>如果曲线发散了，也就是loss呈现增大趋势了，例如这样：<img src="/2022/06/15/pytorch/image-20220629180222356.png" alt="image-20220629180222356" style="zoom:50%;"></p>
<p>那么就说明这次训练是失败的，失败原因有很多，其中最常见的一个原因就是<strong>学习率</strong>取得<strong>太大</strong>了。</p>
<h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>定义：提供n个数据，我们从里面随机的选一个数据，拿单个样本的权重求导，然后对权重进行更新</p>
<p><img src="/2022/06/15/pytorch/image-20220629182943342.png" alt="image-20220629182943342"></p>
<p>换句话说，梯度下降我们是利用了整个函数损失的均值，也就是每一个数据集都参与了计算损失，随机梯度下降就是指从这数据集里的n个数据中随机的取出一个</p>
<p>​        这样做的好处是，即使下降过程中陷入鞍点，我们使用随机梯度，将有可能跨过这个鞍点，向我们的最低值前进</p>
<p>​        <strong>但是</strong>：选点具有随机性，也就是可能会发生与最小点相距很远，或者处于走不出的局部最优的情况</p>
<p>​        因此，让我们分析一下梯度下降法和随机梯度下降法</p>
<ul>
<li>梯度下降法的精度高，但是数据处理需要一定时间，因此时间复杂度相对高</li>
<li>随机梯度下降，精度低，只需处理单个数据，时间复杂度比较低</li>
</ul>
<p>以下是实现的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">	<span class="keyword">return</span> x * w</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">	y_pred = forward(x)</span><br><span class="line">	<span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">x, y</span>):</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">2</span> * x * (x * w - y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">	<span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">		grad = gradient(x, y)</span><br><span class="line">		w = w - <span class="number">0.01</span> * grad</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&quot;\tgrad: &quot;</span>, x, y, grad)</span><br><span class="line">		l = loss(x, y)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;progress:&quot;</span>, epoch, <span class="string">&quot;w=&quot;</span>, w, <span class="string">&quot;loss=&quot;</span>, l)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>所以，对于深度学习而言，我们会在精度和时间复杂度中取一个<code>折中</code></p>
<h3 id="批量随机梯度下降"><a href="#批量随机梯度下降" class="headerlink" title="批量随机梯度下降"></a>批量随机梯度下降</h3><p>​        和计算机组成原理与操作系统一样，经典套娃，我们这里将原有数据集分成若干相同数量的组，每次用这一组样本的mean去更新梯度，我们可以将原有的数据集称为<code>batch</code>,那么这个数据集就叫做<code>Mini-Batch</code>但是我们很多时候都使用这个<code>Mini-Batch</code>，因此在很多接口里就把mini去掉，统称为<strong>batch</strong></p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>​        对于简单的模型，我们可以手动计算导数，然后更新权重，但是对于复杂的模型…<img src="/2022/06/15/pytorch/image-20220701164803478.png" alt="image-20220701164803478"></p>
<p>​        例如，我们总共有5个输入x，他们共在同一维度，我们隐藏层的第一层有六个h，需要从5个x正向传播到6个h，很容易想到我们需要增加权重矩阵，这里顺理成章的增加6×5，共三十个w，以此类推，隐藏第二层需要42个w，第三层需要49个w….因此我们不可能全部都手动生成，故引入反向传播机制</p>
<p><img src="/2022/06/15/pytorch/image-20220701165443312.png" alt="image-20220701165443312"></p>
<p>一个layer代表一层，其中，X为<code>input</code>，MM为<code>Matrix multiplication</code>，ADD为<code>Vector addition</code>，W1为<code>Weight</code>，b1为<code>Bias</code></p>
<p>​        可是这样也会有麻烦，不难看出增加了层数和减少层数，对最终表达式的形式是没有区别的，为了解决这一问题(提高模型的复杂程度)，我们对每一层的输出增加非线性的变化函数，比如sigmoid<img src="/2022/06/15/pytorch/image-20220701165902250.png" alt="image-20220701165902250"></p>
<p>​        简单来说，就是我们对输出的每一个值，都做一个变换，使得式子无法变换成同一个形式，将权值可以进行很好得累加。<img src="/2022/06/15/pytorch/image-20220701170343068.png" alt="image-20220701170343068"></p>
<h3 id="链式求导法则"><a href="#链式求导法则" class="headerlink" title="链式求导法则"></a>链式求导法则</h3><p><img src="/2022/06/15/pytorch/image-20220701171212129.png" alt="image-20220701171212129"></p>
<p>简单来说，链式求导就是循环往复，在复合函数里面进行不断求导，最终得到最外层因变量对最内层自变量的偏导。</p>
<p>在我们反向传播的运用中，首先需要通过每一个单元节点来观察。</p>
<p><img src="/2022/06/15/pytorch/image-20220701171530143.png" alt="image-20220701171530143"></p>
<p>这里假设此时传入f的w为3，x为2，f=x*w，每一个f层都能计算局部的梯度，所以顺理成章传出的z即为6，这个过程我们称为前向传播<code>forward</code>, 后，到传到终止时，此时计算出L对最末隐藏层的导数，以此从后往前，因为每一个参数现在全都已经知晓，故可以轻松不断求导，使得分子分母相消，最终得到目标偏导</p>
<p>​        当然，就实际而言，pytorch总是把局部偏导存到变量里面，而非函数里面，这取决于我们的设定方式。</p>
<p>​        接下来我们来看一下完整的计算图</p>
<p><img src="/2022/06/15/pytorch/image-20220701173041690.png" alt="image-20220701173041690"></p>
<blockquote>
<p>注意，loss就是r^2^，并且反向传播第一步是从^2的function开始的，而不是loss，loss是顺带打印出来的。</p>
</blockquote>
<p>​         ^2的偏导结果是2r，r为-1，因此传回-2，到 -的function，对y_hat求偏导，结果就是1 乘以^2传回来的-2，对*的function，只有对w的偏导，结果为x，因为x=1，所以答案就是1 * 1 * -2，为<code>-2</code></p>
<p>​        我们拿到损失值(loss)以后，通常也要把它打印出来，看看我们最终的w是否是真正收敛后计算出来的w，简言之过程就是这样。</p>
<p>​         </p>
<h3 id="引入Tensor"><a href="#引入Tensor" class="headerlink" title="引入Tensor"></a>引入Tensor</h3><p>Tensor名为<code>张量</code>，是pytorch的重要成员，该类包含两个属性，分别是数据（data）和梯度（gradient）<img src="/2022/06/15/pytorch/image-20220701193302150.png" alt="image-20220701193302150"></p>
<p>一旦我们定义了一个Tensor，我们就可以去建立一个链式的求导计算图</p>
<p>现在，我们在利用反向传播机制来重新优化我们最开始的线性模型</p>
<p><img src="/2022/06/15/pytorch/image-20220701193854315.png" alt="image-20220701193854315"></p>
<p>将w设置为一个Tensor变量，注意一定要加<strong>方括号</strong>，因为Tensor默认是一个类似于<u>多维数组</u>的概念</p>
<p>前向——&gt;损失</p>
<p><img src="/2022/06/15/pytorch/image-20220701194149841.png" alt="image-20220701194149841"></p>
<p>本质是一个构建计算图的过程，不断迭代。 </p>
<h3 id="backward-及其他实用fuc"><a href="#backward-及其他实用fuc" class="headerlink" title="backward()及其他实用fuc"></a>backward()及其他实用fuc</h3><p>pytorch帮助我们很方便实现梯度求解的过程，也就是直接能将链式求导的最终结果返回给最开始调用它的Tensor（这里即w）中的Grad里面</p>
<p>同时一个小贴士，backward()之后，计算图就会被释放了，之前正向的那些权重就不会留下了，所以下一次进行loss计算的时候它会创建一个新的计算图，很明显这样其实有的时候是不好的，但我们在进行计算的时候，可能计算图会是不一样的，我们每次释放之后重构，这是一个非常灵活的方式，这也是pytorch的<em>核心竞争力</em></p>
<p>另外一个注意点，w.grad，grad也是一个Tensor，所以我们不能把grad直接做乘除运算，不然这样本质也是在建立一个计算图，因此我们要取w.grad.data,data虽然也是tensor，但此时不会建立计算图了。</p>
<p>第三个，item()是取得标量（默认int），将tensor变成数值，这也是为了防止产生计算图，或者直接给我们得到相应数据</p>
<p>易错提示（已经错过了=-=）</p>
<p><img src="/2022/06/15/pytorch/image-20220701195346581.png" alt="image-20220701195346581"></p>
<p>sum+=l，l是一个loss函数，但是操纵的是一个张量（w是张量，x*w就是，从而一系列都是=-=）</p>
<p>所以他就会通过+=，不断迭代生成图<img src="/2022/06/15/pytorch/image-20220701195612335.png" alt="image-20220701195612335"></p>
<p>在内存里，随着你迭代的次数，越增越大越增越大，就把内存吃饱了hhh</p>
<p><img src="/2022/06/15/pytorch/image-20220701195701549.png" alt="image-20220701195701549"></p>
<p>所以我们每次需要把值取出来哦</p>
<p>另外另外，每一次做完之后我们都需要对w.grad.data进行更新，这里我们使用w.grad.data.zero()，为何要更新呢？，我们仔细观察</p>
<p><img src="/2022/06/15/pytorch/image-20220701200522529.png" alt="image-20220701200522529"></p>
<p>对于标记①的循环而言，我们每一次都是用所有我们有的数据，从而对w进行推进，让他朝着loss更低的方向前进，此时我们首先使用loss求得单个损失，然后将其backward传给w.grad，backward可以保证计算完后将图清空不占用内存，接着用此时得到的这个w.grad.data*学习率来更新w.data，使得每组xy都可以起到修正w的作用，修正完毕后我们一定要设置w.gard.data为0，因为<strong>backward()是会累加grad的</strong>！！！，他不会自动帮我们清空，一定要注意,这个技巧可以为我们所用！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x_data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">y_data = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br><span class="line">w = torch.tensor([<span class="number">1.</span>])</span><br><span class="line">w.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x*w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (Forward(x)-y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        l = Loss(x, y)</span><br><span class="line">        l.backward()</span><br><span class="line">        w.data -= <span class="number">0.01</span>*w.grad.data.item()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;grad: &#123;&#125; &#123;&#125; &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(x, y, w.data.item()))</span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;process: &#x27;</span>, epoch, <span class="string">&#x27; &#x27;</span>, l.item())</span><br><span class="line">w</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">grad: <span class="number">1</span> <span class="number">2</span> <span class="number">1.0199999809265137</span></span><br><span class="line">grad: <span class="number">2</span> <span class="number">4</span> <span class="number">1.0983999967575073</span></span><br><span class="line">grad: <span class="number">3</span> <span class="number">6</span> <span class="number">1.260688066482544</span></span><br><span class="line">process:  <span class="number">0</span>   <span class="number">7.315943717956543</span></span><br><span class="line">grad: <span class="number">1</span> <span class="number">2</span> <span class="number">1.2754743099212646</span></span><br><span class="line">grad: <span class="number">2</span> <span class="number">4</span> <span class="number">1.333436369895935</span></span><br><span class="line">grad: <span class="number">3</span> <span class="number">6</span> <span class="number">1.4534177780151367</span></span><br><span class="line">process:  <span class="number">1</span>   <span class="number">3.9987640380859375</span></span><br><span class="line">grad: <span class="number">1</span> <span class="number">2</span> <span class="number">1.464349389076233</span></span><br><span class="line">grad: <span class="number">2</span> <span class="number">4</span> <span class="number">1.5072014331817627</span></span><br><span class="line">grad: <span class="number">3</span> <span class="number">6</span> <span class="number">1.5959051847457886</span></span><br><span class="line">process:  <span class="number">2</span>   <span class="number">2.1856532096862793</span></span><br><span class="line">grad: <span class="number">1</span> <span class="number">2</span> <span class="number">1.6039870977401733</span></span><br><span class="line">grad: <span class="number">2</span> <span class="number">4</span> <span class="number">1.635668158531189</span></span><br><span class="line">grad: <span class="number">3</span> <span class="number">6</span> <span class="number">1.7012479305267334</span></span><br><span class="line">process:  <span class="number">3</span>   <span class="number">1.1946394443511963</span></span><br><span class="line">grad: <span class="number">1</span> <span class="number">2</span> <span class="number">1.7072229385375977</span></span><br><span class="line">grad: <span class="number">2</span> <span class="number">4</span> <span class="number">1.7306450605392456</span></span><br><span class="line">grad: <span class="number">3</span> <span class="number">6</span> <span class="number">1.779128909111023</span></span><br><span class="line">process:  <span class="number">4</span>   <span class="number">0.6529689431190491</span></span><br><span class="line">grad: <span class="number">1</span> <span class="number">2</span> <span class="number">1.7835463285446167</span></span><br><span class="line">grad: <span class="number">2</span> <span class="number">4</span> <span class="number">1.8008626699447632</span></span><br><span class="line">grad: <span class="number">3</span> <span class="number">6</span> <span class="number">1.836707353591919</span></span><br><span class="line">process:  <span class="number">5</span>   <span class="number">0.35690122842788696</span></span><br><span class="line">grad: <span class="number">1</span> <span class="number">2</span> <span class="number">1.8399732112884521</span></span><br><span class="line">...</span><br><span class="line">grad: <span class="number">1</span> <span class="number">2</span> <span class="number">1.9999996423721313</span></span><br><span class="line">grad: <span class="number">2</span> <span class="number">4</span> <span class="number">1.9999996423721313</span></span><br><span class="line">grad: <span class="number">3</span> <span class="number">6</span> <span class="number">1.9999996423721313</span></span><br><span class="line">process:  <span class="number">99</span>   <span class="number">9.094947017729282e-13</span></span><br><span class="line">tensor([<span class="number">2.0000</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>​        在之前的学习中，我们其实都是利用python或者仅用Tensor来构建模型，没有用太多pytorch的实质功能，接下来我们仍然通过前面介绍的简单的数据集x=[1, 2, 3], y=[2, 4, 6]，基于pytorch框架，构建出可拓展的、有良好弹性的pytorch框架</p>
<h3 id="pytorch模型框架"><a href="#pytorch模型框架" class="headerlink" title="pytorch模型框架"></a>pytorch模型框架</h3><p>主要分为以下四步:</p>
<ul>
<li>构造数据集，作为构建模型而言，需要统一的输入输出，方便模型有规律的运转</li>
<li>nn.Module重写模型，构建出属于自己的模型（主要用于求得y_hat）</li>
<li>使用pytorch自带的损失函数和迭代器</li>
<li>设定训练周期：前馈、反馈、更新、前馈、反馈、更新….<ul>
<li>前馈算损失，反馈算梯度，更新,利用梯度下降算法给各点更新权重</li>
</ul>
</li>
</ul>
<p><img src="/2022/06/15/pytorch/image-20220702112742527.png" alt="image-20220702112742527"></p>
<h4 id="构建数据集"><a href="#构建数据集" class="headerlink" title="构建数据集"></a>构建数据集</h4><p>由于我们的数据很少，因此可以采用小批量建集法</p>
<p><img src="/2022/06/15/pytorch/image-20220702113247893.png" alt="image-20220702113247893"></p>
<p>​        上半部分是利用numpy构建数据集，由于numpy具有<code>广播</code>机制，例：如果一个3 * 3的矩阵和3 * 1的矩阵不能直接做加法，那么他会把1 * 3按照规律扩容成3 * 3 如图上绿色部分所示</p>
<p>​        下半部分是利用pytorch，我们只需要让他们一一对应，即y_hat和x都是3*1的矩阵，然后权重w，b会自动进行广播，变成3 * 1的矩阵进行数乘</p>
<p><img src="/2022/06/15/pytorch/image-20220702113650774.png" alt="image-20220702113650774"></p>
<p>​        接下来我们将别的变量都构成这种形式，loss和y都应该构建成3 * 1的矩阵<img src="/2022/06/15/pytorch/image-20220702113846464.png" alt="image-20220702113846464"></p>
<p>​        在我们这个例子里，构建数据准备如以下形式<img src="/2022/06/15/pytorch/image-20220702113942659.png" alt="image-20220702113942659"></p>
<p>​        曾经我们需要人工求导数，现在我们就不再考虑人工求了，backward()YYDS，现在我们重点的目标变成构造计算图，也就是我们的第二部，构建模型，一旦构建好将来就可以自动的求出了</p>
<p>​        构建只有两个要求，我们需要知道输入的维度(x)和输出的维度(y_hat)，不过要注意，backward()只能使用标量，所以最后我们的loss矩阵求出需要对他进行求和、求均值之类的处理</p>
<p><img src="/2022/06/15/pytorch/image-20220702115941944.png" alt="image-20220702115941944"></p>
<h4 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h4><p>首先定义一个类–需要继承Module</p>
<p>实现_<strong>init_</strong>()和forward(),构造函数<code>init</code>是初始化对象所要调用的函数，<code>forward</code>是进行前向时所执行的计算</p>
<p>不难发现并没有backward()，这是因为用Module构造出来的对象会自动根据计算图帮助我们实现backward()的构成</p>
<p>万一pytorch的backward()无法实现你的高端计算，你可以自定义一个pytorch.function类实现反向传播，构建自己的计算块 </p>
<p><img src="/2022/06/15/pytorch/image-20220702120735568.png" alt="image-20220702120735568"></p>
<h5 id="linear-参数解释"><a href="#linear-参数解释" class="headerlink" title="linear()参数解释"></a>linear()参数解释</h5><p>这是linear()的参数详细解释</p>
<p><img src="/2022/06/15/pytorch/image-20220702144708222.png" alt="image-20220702144708222"></p>
<h5 id="call-方法与自定传参"><a href="#call-方法与自定传参" class="headerlink" title="call()方法与自定传参"></a>call()方法与自定传参</h5><p><img src="/2022/06/15/pytorch/image-20220702144234536.png" alt="image-20220702144234536"></p>
<p>可以看见我们写成*args的形式，则会将传入参数存为元组，写成**kwargs可会将传入的参数存在词典</p>
<p>call()中会去调用forward()，所以我们在我们的方法中必须去实现forward()，重写覆盖掉父类函数 </p>
<h4 id="构建迭代器和损失函数"><a href="#构建迭代器和损失函数" class="headerlink" title="构建迭代器和损失函数"></a>构建迭代器和损失函数</h4><p><strong>损失函数</strong></p>
<p><img src="/2022/06/15/pytorch/image-20220702145202356.png" alt="image-20220702145202356"></p>
<p><img src="/2022/06/15/pytorch/image-20220702145228051.png" alt="image-20220702145228051"></p>
<p>一般来说只考虑size_average是否需要设置，一般来说都可以，影响不大，就是需不需要求均值。False就是叠加loss</p>
<p> 需要的参数就是y_hat和y</p>
<p><strong>优化器</strong></p>
<p><img src="/2022/06/15/pytorch/image-20220702145829546.png" alt="image-20220702145829546"><img src="/2022/06/15/pytorch/image-20220702152106023.png" alt="image-20220702152106023"></p>
<p>model.parameters()类似于你无论使用多么复杂的模型，它都能把你的各种模型的参数找到，lr就是学习率，目前是固定的，在pytorch里面甚至可以对模型的不同板块做不同的学习率处理</p>
<h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><ol>
<li>在model的forward中算y_hat</li>
<li>利用criterion算loss</li>
<li>利用optimizer使得训练的权重都清零（特指上一次训练的权重，注意1、2步都并未对optimizer进行操作，而下一步的backward()则叠加梯度）</li>
<li>进行反向传播backward()</li>
<li>利用step()进行更新，会根据所有参数包含的梯度以及学习率进行更新</li>
</ol>
<p><img src="/2022/06/15/pytorch/image-20220702152554905.png" alt="image-20220702152554905"></p>
<p>这个训练就等价于之前</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">		grad = gradient(x, y)</span><br><span class="line">		w = w - <span class="number">0.01</span> * grad</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&quot;\tgrad: &quot;</span>, x, y, grad)</span><br><span class="line">		l = loss(x, y)</span><br></pre></td></tr></table></figure>

<p>的过程</p>
<h4 id="打印结果"><a href="#打印结果" class="headerlink" title="打印结果"></a>打印结果</h4><p>最后打印训练后的参数</p>
<p><img src="/2022/06/15/pytorch/image-20220702152852893.png" alt="image-20220702152852893"></p>
<p>拿出来的值</p>
<p><img src="/2022/06/15/pytorch/image-20220702152928940.png" alt="image-20220702152928940"></p>
<p>model内的linear，linear内的weight</p>
<p>x_test使用二维矩阵，意思是1*1的矩阵，</p>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.</span>], [<span class="number">2.</span>], [<span class="number">3.</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.</span>], [<span class="number">4.</span>], [<span class="number">6.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">linearModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(linearModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">model = linearModel()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.04</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    y_pred = model.forward(x_data)</span><br><span class="line">    loss = criterion(y_data, y_pred)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w= &#x27;</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b= &#x27;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_test= &#x27;</span>, y_test)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span> <span class="number">10.964780807495117</span></span><br><span class="line"><span class="number">1</span> <span class="number">9.638935089111328</span></span><br><span class="line"><span class="number">2</span> <span class="number">8.408780097961426</span></span><br><span class="line"><span class="number">3</span> <span class="number">7.27571439743042</span></span><br><span class="line"><span class="number">4</span> <span class="number">6.240664005279541</span></span><br><span class="line"><span class="number">5</span> <span class="number">5.304004669189453</span></span><br><span class="line"><span class="number">6</span> <span class="number">4.465476989746094</span></span><br><span class="line"><span class="number">7</span> <span class="number">3.7241055965423584</span></span><br><span class="line"><span class="number">8</span> <span class="number">3.0781075954437256</span></span><br><span class="line"><span class="number">9</span> <span class="number">2.524812698364258</span></span><br><span class="line"><span class="number">10</span> <span class="number">2.0606095790863037</span></span><br><span class="line"><span class="number">11</span> <span class="number">1.6808964014053345</span></span><br><span class="line"><span class="number">12</span> <span class="number">1.3800746202468872</span></span><br><span class="line"><span class="number">13</span> <span class="number">1.1515748500823975</span></span><br><span class="line"><span class="number">14</span> <span class="number">0.987949788570404</span></span><br><span class="line"><span class="number">15</span> <span class="number">0.8810074925422668</span></span><br><span class="line"><span class="number">16</span> <span class="number">0.8220123052597046</span></span><br><span class="line"><span class="number">17</span> <span class="number">0.8019364476203918</span></span><br><span class="line"><span class="number">18</span> <span class="number">0.8117435574531555</span></span><br><span class="line"><span class="number">19</span> <span class="number">0.8427058458328247</span></span><br><span class="line"><span class="number">20</span> <span class="number">0.886695384979248</span></span><br><span class="line"><span class="number">21</span> <span class="number">0.9364582300186157</span></span><br><span class="line"><span class="number">22</span> <span class="number">0.9858223795890808</span></span><br><span class="line"><span class="number">23</span> <span class="number">1.0298420190811157</span></span><br><span class="line"><span class="number">24</span> <span class="number">1.064853549003601</span></span><br><span class="line">...</span><br><span class="line"><span class="number">999</span> <span class="number">4.547473508864641e-13</span></span><br><span class="line">w=  <span class="number">1.9999995231628418</span></span><br><span class="line">b=  <span class="number">9.625380243960535e-07</span></span><br><span class="line">y_test=  tensor([[<span class="number">8.0000</span>]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h2 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h2><blockquote>
<p>一个有意思的模型，虽然叫做回归，却用来做分类任务</p>
</blockquote>
<h3 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h3><p>深度学习的hello world！ <code>MNIST</code>数据集,总共有十个类，0-9数字</p>
<p><img src="/2022/06/15/pytorch/image-20220703112453579.png" alt="image-20220703112453579"></p>
<p>​        我们考虑做这个分类任务，并不能考虑一维实数空间数值大小的含义啥的，就好像7和9长得反而像一点，但中间却隔了一个8，相反，我们应该对当前输入的图片，他长得像啥，对十个类别分别学习出十个概率，然后选择这十个概率中最大概率的一类作为他应该归入的一类。</p>
<p><code>CIFAR</code>数据集为32*32的小图片，10个分类，十种动物</p>
<p><img src="/2022/06/15/pytorch/image-20220703113259355.png" alt="image-20220703113259355"></p>
<p>使用的参数和MNIST近似，模仿着做法即可下载</p>
<h3 id="logistic函数"><a href="#logistic函数" class="headerlink" title="logistic函数"></a>logistic函数</h3><p>​        回归任务y的2,4,6就表示它将来能拿到的分数，而分类任务就是True和False，仅仅看得是他能否通过考试，看起来二分类任务是计算两个值，实际上是计算一个值，因为二者和为1，但如果两种可能都是近似0.5，那么就是说学习器没有十足的把握判断，我们可以加一句判断条件，就是说在0.4~0.6区间内，给出具体概率，提示用户，我们不确定，因为报错总比出错好。 </p>
<p>​        对于线性模型来说，输出属于实数范围内，但我们现在要得分类的输出是一个概率，所以我们这个输出应该属于0~1这个区间内，所以我们需要把输出值从实数空间映射到[0, 1]</p>
<p><img src="/2022/06/15/pytorch/image-20220703114751449.png" alt="image-20220703114751449"></p>
<p>​        我们不难看出，他越往0靠或者越往1靠的时候，他的变化是不大的，但是0.5左右波动很大，这样的称为饱和函数，这非常满足我们做分类任务的需求。logistic的导数长成这样</p>
<p><img src="/2022/06/15/pytorch/image-20220703115041163.png" alt="image-20220703115041163"></p>
<p>非常的符合正态分布，这也解释了为什么logistic是由于正态分布的存在而产生的的一种函数。</p>
<p>​        注意这里并不是说有logistic函数就能进行概率转换，而是说我们想要计算概率，<strong>必须保证</strong>我们的输出值在0~1之间</p>
<img src="/2022/06/15/pytorch/image-20220703115336845.png" alt="image-20220703115336845" style="zoom:50%;">

<h3 id="饱和函数"><a href="#饱和函数" class="headerlink" title="饱和函数"></a>饱和函数</h3><p>所有的饱和函数都满足：</p>
<ol>
<li>函数值有极限</li>
<li>都是单调的<strong>增</strong>函数</li>
<li>都是饱和函数</li>
</ol>
<p>只要满足这三个条件都叫做sigmoid函数，其中，logistic最出名，所以有的框架中我们直接把它称作sigmoid函数</p>
<p>当然，logistic的函数收敛是在0~ 1，有的时候我们可能更希望它在-1~1或者其他区间,这个时候就要用tanh或者其他啥函数</p>
<p><img src="/2022/06/15/pytorch/image-20220703115617555.png" alt="image-20220703115617555"></p>
<h3 id="与线性模型对比"><a href="#与线性模型对比" class="headerlink" title="与线性模型对比"></a>与线性模型对比</h3><h4 id="方程式"><a href="#方程式" class="headerlink" title="方程式"></a>方程式</h4><p><img src="/2022/06/15/pytorch/image-20220703115838306.png" alt="image-20220703115838306"></p>
<p>凡是有这个歪着的6，我们就把他看做是logistic回归的符号</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><img src="/2022/06/15/pytorch/image-20220703120342435.png" alt="image-20220703120342435"></p>
<p>​        我们以前的loss是用来计算欧式距离的，而现在我们的输出不再是一个数值的，我们输出是一个分布，分布不是数轴上的距离，这里我们引入交叉熵（Cross-entropy）的概念</p>
<p>​        假设我们现在有两个分布，一个P<del>D</del>一个P<del>T</del> </p>
<p><img src="/2022/06/15/pytorch/image-20220703134119590.png" alt="image-20220703134119590"></p>
<p>​        我们可以用这个式子表示两个分布之间的差异，并希望这个求和的结果越大越好（ln0.x的结果是为负的），但为了和MSE，MAE等等统一标准，所以我们在最前面加一个负号，使得结果越小是越好的</p>
<p><img src="/2022/06/15/pytorch/image-20220703135211514.png" alt="image-20220703135211514"></p>
<p>​        现在，我们来进一步分析一下交叉熵是如何做到，辨别分类准确情况的。首先需要明确一点，y的取值只有0和1，因为这是二分类问题，其次，当y为1时，原式变为了-ylog^y_hat^，想要这个式子最小，那么y_hat就得越大越好，所以说越趋近于1，预测值就越准确，同时，1就是该的分类物品所占有的对应数字，0也是同样的道理，这样就使得预测的东西越与标准答案接近，loss越低，从而达到我们想要的目的。这个函数，被我们称为<code>BCE</code></p>
<p><img src="/2022/06/15/pytorch/image-20220703135800503.png" alt="image-20220703135800503"></p>
<p>​        这就是一个具体实例，对于有多个样本的BCE，直接求均值就好</p>
<h4 id="代码对比"><a href="#代码对比" class="headerlink" title="代码对比"></a>代码对比</h4><p><img src="/2022/06/15/pytorch/image-20220703135950349.png" alt="image-20220703135950349"></p>
<p>其实就多了一个sigmoid嵌套就行，σ本身不需要参数，不需要在构造函数里初始化。</p>
<p><img src="/2022/06/15/pytorch/image-20220703140331544.png" alt="image-20220703140331544"></p>
<p>是否求均值，影响学习率的设置。因为你的函数损失变小了，你将来求出来的导数也会乘以这个常数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare dataset</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.</span>], [<span class="number">2.</span>], [<span class="number">3.</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Design model using Class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.sigmoid(self.linear(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct loss and optimizer</span></span><br><span class="line">model = LogisticRegressionModel()</span><br><span class="line">criterion = torch.nn.BCELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training cycle</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    y_pred = model.forward(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch&#x27;</span>, epoch, <span class="string">&#x27;loss=&#x27;</span>, loss.item())</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot graph</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">200</span>)</span><br><span class="line">x_t = torch.Tensor(x).view((<span class="number">200</span>, <span class="number">1</span>))</span><br><span class="line">y_t = model(x_t)</span><br><span class="line">y = y_t.data.numpy()</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">10</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>], c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Hours&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Probability of Pass&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">epoch <span class="number">0</span> loss= <span class="number">3.4830539226531982</span></span><br><span class="line">epoch <span class="number">1</span> loss= <span class="number">3.4443485736846924</span></span><br><span class="line">epoch <span class="number">2</span> loss= <span class="number">3.405913829803467</span></span><br><span class="line">epoch <span class="number">3</span> loss= <span class="number">3.367762804031372</span></span><br><span class="line">epoch <span class="number">4</span> loss= <span class="number">3.329906702041626</span></span><br><span class="line">epoch <span class="number">5</span> loss= <span class="number">3.292354106903076</span></span><br><span class="line">epoch <span class="number">6</span> loss= <span class="number">3.255119800567627</span></span><br><span class="line">...</span><br><span class="line">epoch <span class="number">996</span> loss= <span class="number">0.7208291292190552</span></span><br><span class="line">epoch <span class="number">997</span> loss= <span class="number">0.7202318906784058</span></span><br><span class="line">epoch <span class="number">998</span> loss= <span class="number">0.7196354866027832</span></span><br><span class="line">epoch <span class="number">999</span> loss= <span class="number">0.7190396189689636</span></span><br></pre></td></tr></table></figure>

<p><img src="/2022/06/15/pytorch/output-16568300183541.png" alt="output"></p>
<p>不难看出，达到0.5可能的点位是2.5，我们可以深度思考一下他的原因，其实最主要是因为，我们x=1,2，的概率都是设置的y=0；x=3的概率设置的是y=1，所以自然在x=2.5的位置就达到了50%左右的可能性</p>
<h3 id="多维输入"><a href="#多维输入" class="headerlink" title="多维输入"></a>多维输入</h3><h4 id="数据集解释"><a href="#数据集解释" class="headerlink" title="数据集解释"></a>数据集解释</h4><p><img src="/2022/06/15/pytorch/image-20220703150157174.png" alt="image-20220703150157174"></p>
<p>x1，x2，x3…..y我们可以把它看做不同的特征，每一组特征（feature）称为一个样本（sample）</p>
<h4 id="清晰的推导"><a href="#清晰的推导" class="headerlink" title="清晰的推导"></a>清晰的推导</h4><p><img src="/2022/06/15/pytorch/image-20220703145823639.png" alt="image-20220703145823639"></p>
<p>​        我们可以把logistic回归的方程转换为可以升维的形式，这里使用输入八维来举例</p>
<p><img src="/2022/06/15/pytorch/image-20220703150420540.png" alt="image-20220703150420540"></p>
<p>​        注意观察图中式子的变换，很有启发意义，线代的知识又捡回来了！！！</p>
<p>​        为什么我们一定要转化为矩阵运算呢？矩阵转换为向量化的计算后，我们可以利用并行计算的能力（GPU、CPU）来提高整个运算的速度，如果用for循环来写一定是相当慢的</p>
<p>​        对代码的修改而言，我们需要做的仅仅只有把代码里面线性的(1, 1)改成(8, 1)就可以了</p>
<img src="/2022/06/15/pytorch/image-20220703150823967.png" alt="image-20220703150823967" style="zoom:50%;">

<p>另外就是，x输入，我们只需要改成8 * 1的Tensor， y继续1 * 1即可</p>
<h4 id="降维处理"><a href="#降维处理" class="headerlink" title="降维处理"></a>降维处理</h4><p><img src="/2022/06/15/pytorch/image-20220703151852543.png" alt="image-20220703151852543"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)</span><br><span class="line">x_data = torch.from_numpy(xy[:,:-<span class="number">1</span>])</span><br><span class="line">y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">model</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigmoid = torch.nn.Sigmoid()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.sigmoid(self.linear1(x)) <span class="comment"># x -&gt; O1</span></span><br><span class="line">        x = self.sigmoid(self.linear2(x)) <span class="comment"># x -&gt; O2</span></span><br><span class="line">        x = self.sigmoid(self.linear3(x)) <span class="comment"># x -&gt; Y_hat</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">model = model()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.BCELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr = <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span> <span class="number">499.0527038574219</span></span><br><span class="line"><span class="number">1</span> <span class="number">489.77032470703125</span></span><br><span class="line"><span class="number">2</span> <span class="number">493.4767761230469</span></span><br><span class="line"><span class="number">3</span> <span class="number">493.3719787597656</span></span><br><span class="line"><span class="number">4</span> <span class="number">490.6575927734375</span></span><br><span class="line">...</span><br><span class="line"><span class="number">9996</span> <span class="number">258.5865478515625</span></span><br><span class="line"><span class="number">9997</span> <span class="number">258.5833435058594</span></span><br><span class="line"><span class="number">9998</span> <span class="number">258.5801696777344</span></span><br><span class="line"><span class="number">9999</span> <span class="number">258.5771789550781</span></span><br></pre></td></tr></table></figure>

<h2 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h2><h3 id="概念说明"><a href="#概念说明" class="headerlink" title="概念说明"></a>概念说明</h3><p>我们使用数据集，即把已有的整个batch构成mini-batch，分别有<code>Epoch</code>, <code>Batch-Size</code>, <code>Iterations</code>.</p>
<ul>
<li>Epoch：当所有的样本，都进行了正向传播和反向传播<strong>一次</strong>，那么这个过程就叫做Epoch</li>
<li>Batch-Size：每次训练时所用的样本数量，也就是每进行<strong>一次</strong>前馈和反馈所用的</li>
<li>Iteration： 在一个Epoch内，样本迭代的次数，也就是Batch-Size的总数</li>
</ul>
<blockquote>
<p>Batch = Mini-Batch- * Iteration</p>
</blockquote>
<p>  <img src="/2022/06/15/pytorch/image-20220703161717162.png" alt="image-20220703161717162"></p>
<p>shuffle为True，意味着打乱顺序，Loader按照每个mini-batch的尺寸为2，装载数据集</p>
<h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><p>首先有以下几个注意点</p>
<ol>
<li>Dataset是一个抽象类，他在实现的时候需要继承（例如DiabetesDataset(Dataset）)这感觉就和构建模型继承torch.nn.Module一样，我们这样做了以后，从而构造自定义的数据集</li>
<li>Dataloader是用来加载数据的，我们实例化一个对象来帮我们做就是，他的参数中，dataset为数据集，batch_size为minibatch尺寸，shuffle就是洗牌，num_workers表示到底要几个并行的进程去读取你的数据</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiabetesDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">dataset = DiabetesDataset()</span><br><span class="line">train_loader = DataLoader(dataset=dataset,</span><br><span class="line">                            batch_size=<span class="number">32</span>,</span><br><span class="line">                            shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>这里还要提醒一些构造数据集的注意点</p>
<ol>
<li>我们可以把所有数据都先加载到内存里面，然后每次使用getitem()的时候，把对应的数据[i]传出去就好了，这种适用于数据集本身的容量不大。</li>
<li>如果处理，图像，语言等非结构的数据，我们要考虑能不能把它加载到内存当中，通常这会涉及到列表和标签，如果标签比较少，类似于也是一个回归问题，我们也可以把它都读进来，如果标签也是一个特别复杂的张量，那么我们就把标签也放到一个列表里面，然后在getitem()里面读取列表的第i个文件，分别把标签和数据本身的第i个文件返回，保证内存的高效使用。<ol>
<li>就是说用文件的文件名作为文件内容的索引，读到内存中，等到用的时候，在用文件名去读文件内容。</li>
</ol>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiabetesDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, filepath</span>):</span></span><br><span class="line">        xy = np.loadtxt(filepath, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)</span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        self.x_data = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">        self.y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.x_data[index], self.y_data[index]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line">dataset = DiabetesDataset(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>)</span><br><span class="line">train_loader = DataLoader(dataset=dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line"><span class="comment"># 1. Prepare data</span></span><br><span class="line">inputs, labels = data</span><br><span class="line"><span class="comment"># 2. Forward</span></span><br><span class="line">y_pred = model(inputs)</span><br><span class="line">loss = criterion(y_pred, labels)</span><br><span class="line"><span class="built_in">print</span>(epoch, i, loss.item())</span><br><span class="line"><span class="comment"># 3. Backward</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment"># 4. Update</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>

<h2 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h2><p>​        我们在多维输入里介绍了这个hello world，现在我们就来考虑如何实现它</p>
<p><img src="/2022/06/15/pytorch/image-20220703191038082.png" alt="image-20220703191038082"></p>
<p>对于这个神经网络，我们引入Softmax层。它能使得输出的每个概率满足累加起来为1，并且均＞0的条件</p>
<p><img src="/2022/06/15/pytorch/image-20220703191702113.png" alt="image-20220703191702113"></p>
<h3 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h3><p><img src="/2022/06/15/pytorch/image-20220703192103146.png" alt="image-20220703192103146"></p>
<p>​        引入e为底，这样可以使得，不管线性层最后是正是负，终能达到如上所需要求，具体的算法描述已经在老师的白板中显示的很清楚了，将a，b，c看做三个分类就行，也就是Z<del>0</del>,Z<del>1</del>,Z<del>2</del>。</p>
<p>​        那么为何用e为底数呢？最简单的理解就是这个函数会相应扩大最大概率值的比重，有利于分类</p>
<p><img src="/2022/06/15/pytorch/image-20220703192540687.png" alt="image-20220703192540687"></p>
<pre><code>     现在，我们得出了当前输入，属于三个分类可能的分别的概率
</code></pre>
<p>再考虑我们的交叉熵<img src="/2022/06/15/pytorch/image-20220703192949489.png" alt="image-20220703192949489"></p>
<p>意味着最后只有一个可以输出。</p>
<p>​        我们要算损失，应该怎么来实现呢？</p>
<p>这就要引入经久不衰的one-hot了！</p>
<h3 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h3><p><img src="/2022/06/15/pytorch/image-20220703193401634.png" alt="image-20220703193401634"></p>
<p>NLLLoss，y输入的就是标签号，0,1,2,3，但他要求你的y_hat需要在计算完后先自行log再放入，他不会帮你计算交叉熵公式中的log</p>
<p><img src="/2022/06/15/pytorch/image-20220703193538531.png" alt="image-20220703193538531"></p>
<p> 过程看着其实挺繁琐，but，pytorchYYDS</p>
<p><img src="/2022/06/15/pytorch/image-20220703193706444.png" alt="image-20220703193706444"></p>
<p>​        CrossEntropyLoss()帮你把一切封装好，甚至都不需要激活层softmax，一键生成损失！</p>
<p>好用如他也要注意：<strong>分类标准y需要是长整型张量</strong>，这里是指，标准分类仅有0，然后看z和[[0, 0, 0]]的交叉熵</p>
<pre><code>     在举一个实例，加深理解：
</code></pre>
<p><img src="/2022/06/15/pytorch/image-20220703194333608.png" alt="image-20220703194333608"></p>
<p>​        Y为标准分类类别，2,0,1。 对于Y_pred1内，我们横向看，总共有3组训练好的预测结果，是没有经过softmax的，很轻松看出这三组的概率，分别第二个元素，第零个元素，第一个元素最大，和标准分类结果吻合，所以右边loss1也明显很小，Y_pred2同样方式，看出是0，2,2，极度不搭边，所以loss偏大</p>
<p>​        最后，两者都需要搞清楚适用场景，两者都有合适的领域。</p>
<p><img src="/2022/06/15/pytorch/image-20220703194720018.png" alt="image-20220703194720018"></p>
<h3 id="MNIST-Solution"><a href="#MNIST-Solution" class="headerlink" title="MNIST Solution"></a>MNIST Solution</h3><p><img src="/2022/06/15/pytorch/image-20220703195850902.png" alt="image-20220703195850902"></p>
<p>对于单个数字的图片，我们先来感受一下</p>
<p><img src="/2022/06/15/pytorch/image-20220703200049826.png" alt="image-20220703200049826"></p>
<p>​        这次，我们要加入测试集，用一部分数据来看看我们的训练效果到底怎么样</p>
<p>这是实现了的代码，对于第一个部分transforms的ToTensor()，我们可以先来理解一下他为什么要这样做</p>
<p><img src="/2022/06/15/pytorch/image-20220703200753754.png" alt="image-20220703200753754"></p>
<p>首先引入通道的概念</p>
<p>​        在我们的视觉中，看上去每一个灰度图像，他实际叫做单通道，有单通道就有多通道，其中高为<code>H</code>，宽为<code>W</code>，通道层数为<code>Channel</code></p>
<p>图像张量，一般都是w * h * c，在pytorch需要转化为c * w * h,这是为了在pytorch里进行更高效的卷积运算</p>
<p><img src="/2022/06/15/pytorch/image-20220703201008265.png" alt="image-20220703201008265"></p>
<p>​        所以在这里，我们要把28*28的像素值先扩张成1 * 28 * 28的单通道图像，并把【0-255】的值压缩成【0-1】的浮点数，这个过程就可以通过ToTensor()来实现</p>
<p><img src="/2022/06/15/pytorch/image-20220703201819948.png" alt="image-20220703201819948"></p>
<p>​        后我们使用Normalize来进行标准化操作，这两个值是提前算出的方差和标准差。</p>
<img src="/2022/06/15/pytorch/image-20220703202037843.png" alt="image-20220703202037843">

<p>MNIST调用的transform()可以放到getitem()里面来实现，也可以就在这里使用。</p>
<p><img src="/2022/06/15/pytorch/image-20220703202541556.png" alt="image-20220703202541556"></p>
<p>​        这里使用线性模型配套激活函数不断调整权重，为何要降低这么多次维度，而不直接降到10，这主要是因为这样的话丢掉的信息太多，训练出来的效果并不好，另外要注意的一个小点，是最后的output层，我们要对他进行交叉熵函数，所以不需要激活。</p>
<p><img src="/2022/06/15/pytorch/image-20220703202942786.png" alt="image-20220703202942786"></p>
<p>​        momentum，给数据一个惯性集，让他在鞍点走出来，让他加速，助推！</p>
<h3 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader <span class="comment"># 注意大小写</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(), <span class="comment"># 将图像转化为图像张量（自动完成）pil image-&gt; tensor</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.1307</span>, ), (<span class="number">0.3081</span>, ))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>,</span><br><span class="line">                                train=<span class="literal">True</span>,</span><br><span class="line">                                download=<span class="literal">True</span>,</span><br><span class="line">                                transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset,</span><br><span class="line">                            shuffle=<span class="literal">True</span>,</span><br><span class="line">                            batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>,</span><br><span class="line">                                train=<span class="literal">False</span>,</span><br><span class="line">                                download=<span class="literal">True</span>,</span><br><span class="line">                                transform=transform)</span><br><span class="line">test_loader = DataLoader(train_dataset,</span><br><span class="line">                            shuffle=<span class="literal">False</span>,</span><br><span class="line">                            batch_size=batch_size)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.l1 = torch.nn.Linear(<span class="number">784</span>, <span class="number">512</span>)</span><br><span class="line">        self.l2 = torch.nn.Linear(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.l3 = torch.nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.l4 = torch.nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.l5 = torch.nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x.reshape(-<span class="number">1</span>, <span class="number">784</span>)</span><br><span class="line">        x = F.relu(self.l1(x))</span><br><span class="line">        x = F.relu(self.l2(x))</span><br><span class="line">        x = F.relu(self.l3(x))</span><br><span class="line">        x = F.relu(self.l4(x))</span><br><span class="line">        x = F.relu(self.l5(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">model = Net()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum = <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    running_loss = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        output = model(inputs)</span><br><span class="line">        loss = criterion(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span>  == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch+<span class="number">1</span>, batch_idx+<span class="number">1</span>, running_loss/<span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():    <span class="comment"># 无需梯度，不要loss</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, axis = <span class="number">1</span>) <span class="comment"># _代表不需要这个值返回,predicted为10种类中的最大概率</span></span><br><span class="line">            total += labels.size(<span class="number">0</span>) <span class="comment">#统计第0维度的个数，这个labels是64个，batch-size！！！！dataloader每取出一组，就是64个！</span></span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item() <span class="comment"># 多少个猜对了</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy on test set: %d %%&#x27;</span> % (<span class="number">100</span> * correct/total))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>: <span class="comment"># 是一个标志，类似象征着Java等语言中的程序主入口，告诉其他程序员，代码入口在此</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span>,   <span class="number">300</span>] loss: <span class="number">0.473</span></span><br><span class="line">[<span class="number">1</span>,   <span class="number">600</span>] loss: <span class="number">0.476</span></span><br><span class="line">[<span class="number">1</span>,   <span class="number">900</span>] loss: <span class="number">0.465</span></span><br><span class="line">Accuracy on test <span class="built_in">set</span>: <span class="number">80</span> %</span><br><span class="line">[<span class="number">2</span>,   <span class="number">300</span>] loss: <span class="number">0.461</span></span><br><span class="line">[<span class="number">2</span>,   <span class="number">600</span>] loss: <span class="number">0.468</span></span><br><span class="line">[<span class="number">2</span>,   <span class="number">900</span>] loss: <span class="number">0.469</span></span><br><span class="line">....</span><br><span class="line">Accuracy on test <span class="built_in">set</span>: <span class="number">80</span> %</span><br><span class="line">[<span class="number">10</span>,   <span class="number">300</span>] loss: <span class="number">0.436</span></span><br><span class="line">[<span class="number">10</span>,   <span class="number">600</span>] loss: <span class="number">0.444</span></span><br><span class="line">[<span class="number">10</span>,   <span class="number">900</span>] loss: <span class="number">0.451</span></span><br><span class="line">Accuracy on test <span class="built_in">set</span>: <span class="number">80</span> %</span><br></pre></td></tr></table></figure>

<p>其实我也不是很明白为啥80%，可能我的cpu太菜了</p>
<h2 id="CNN初阶"><a href="#CNN初阶" class="headerlink" title="CNN初阶"></a>CNN初阶</h2><p>每一个输入节点都要参与到下一层任何一个输出结点的计算上，我们把这样的一层叫做全连接层</p>
<h3 id="构建卷积层"><a href="#构建卷积层" class="headerlink" title="构建卷积层"></a>构建卷积层</h3><p>先放上整体过程，后续介绍细节。</p>
<p><img src="/2022/06/15/pytorch/image-20220706175607025.png" alt="image-20220706175607025"></p>
<p>Convolution</p>
<pre><code>     我们在计算机中，所使用到的都是RGB颜色的通道图像，我们要表示一个像素，把它分成若干个格子，每一个格子有一个颜色值，用它们来表示这个图像，我们把这样的图像叫做栅格图像。
</code></pre>
<p><img src="/2022/06/15/pytorch/image-20220706181328910.png" alt="image-20220706181328910"></p>
<p>通常彩色图片是三通道</p>
<p>​        我们将猫猫的图像信息输入，很显然，输入信息是3 * w * h的</p>
<p>假设下一层output cannel为 3 * 1 * 1，然后我们就取一个patch中的数据，对他们进行压缩，使得ouput的每一个1，都包含对应patch-size的原图像的信息，最终不断运用这些信息进行调整，得到我们想要的结果</p>
<p><img src="/2022/06/15/pytorch/image-20220706190842111.png" alt="image-20220706190842111"></p>
<p>​        卷积运算，通过选择对应的batch-size和对应的kernel-size，后对其做数乘处理，并将累加结果放入output的第一个位置</p>
<p><img src="/2022/06/15/pytorch/image-20220706191010883.png" alt="image-20220706191010883"></p>
<p>​        若有多个维度，可以构建多个卷积核，对同样的样本做卷积数乘运算，运算以后的output对应位置叠加，即可得到最后的一层output</p>
<p><img src="/2022/06/15/pytorch/image-20220706191311290.png" alt="image-20220706191311290"></p>
<p>​        若我们要得到m个输出通道，那我们就准备m个卷积核，把这m个output进行cat，后就可以变成一个m * w * h的张量</p>
<p>对于每个卷积核大小的确定，我们可以发现：</p>
<ul>
<li>每一个卷积核的通道数量要求和输入通道（n）一样</li>
<li>卷积核的总数有多少个，和输出通道的数量（m）一样</li>
</ul>
<p><img src="/2022/06/15/pytorch/image-20220706191809144.png" alt="image-20220706191809144"></p>
<p>整个卷积过程的尺寸，我们可以用四维张量来表示</p>
<p><img src="/2022/06/15/pytorch/image-20220706192013058.png" alt="image-20220706192013058"></p>
<h4 id="代码实现-3"><a href="#代码实现-3" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">in_channels, out_channels= <span class="number">5</span>, <span class="number">10</span></span><br><span class="line">width, height = <span class="number">100</span>, <span class="number">100</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成时取随机数举例，测试使用而已</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(batch_size,     <span class="comment"># B</span></span><br><span class="line">                    in_channels,    <span class="comment"># n</span></span><br><span class="line">                    width,          <span class="comment"># w</span></span><br><span class="line">                    height)         <span class="comment"># H</span></span><br><span class="line"></span><br><span class="line">conv_layer = torch.nn.Conv2d(in_channels,</span><br><span class="line">                            out_channels,   <span class="comment"># 卷积层必须要有输入输出层和内核尺寸</span></span><br><span class="line">                            kernel_size=kernel_size)    <span class="comment"># 我们这输入一个3，意思就是3*3，若要长方形，使用(5, 2)元祖的形式即可 </span></span><br><span class="line">                                                        <span class="comment"># 通常情况卷积层是基数(为了补0方便)，但对于pytorch奇偶数无所谓啦</span></span><br><span class="line">output = conv_layer(<span class="built_in">input</span>)  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"><span class="built_in">print</span>(conv_layer.weight.shape)      <span class="comment"># 输出通道 输入通道</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">1</span>, <span class="number">5</span>, <span class="number">100</span>, <span class="number">100</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">10</span>, <span class="number">98</span>, <span class="number">98</span>])</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<h4 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h4><p>​        卷积层初始化时的常见参数之一，  padding的目的是填充补齐，比如说input和卷积层大小已经确定，例如就是5 * 5和3 * 3，那么最后得到的output肯定就是3 * 3（这里步长默认都是1）。如果我们不想让他是3 * 3，或者说，最后维度和input一样, 应该怎么办呢？默认来说，就是给input周围补0，使得卷积后的结果能是5 * 5。</p>
<blockquote>
<p>在这个过程中我们也发现了一些公式，目前目标仍然是使得output和input维度相同，做padding，看kernel是多少。这个时候我们通常用kernel_size / 2（整除哈）得到多少，那么就需要在外面padding多少层，例如下图是3*3的话，就需要填充3 / 2 = 1层</p>
</blockquote>
<p><img src="/2022/06/15/pytorch/image-20220706194233779.png" alt="image-20220706194233779"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">input</span> = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">5</span>,<span class="number">7</span>, </span><br><span class="line">         <span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">2</span>, </span><br><span class="line">         <span class="number">1</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">4</span>, </span><br><span class="line">         <span class="number">9</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">2</span>, </span><br><span class="line">         <span class="number">3</span>,<span class="number">7</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="built_in">input</span> = torch.Tensor(<span class="built_in">input</span>).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>) <span class="comment"># B C W H</span></span><br><span class="line">conv_layer = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>) <span class="comment"># bias偏移量</span></span><br><span class="line">kernel = torch.Tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>) <span class="comment"># Output Input W H</span></span><br><span class="line">conv_layer.weight.data = kernel.data <span class="comment"># 自己定义了卷积层的初始化，将其赋给之前定义好的conv_layer</span></span><br><span class="line">output = conv_layer(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<h4 id="stride"><a href="#stride" class="headerlink" title="stride"></a>stride</h4><p>步长。</p>
<p><img src="/2022/06/15/pytorch/image-20220706194751841.png" alt="image-20220706194751841"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">input</span> = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">5</span>,<span class="number">7</span>, </span><br><span class="line">         <span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">2</span>, </span><br><span class="line">         <span class="number">1</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">4</span>, </span><br><span class="line">         <span class="number">9</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">2</span>, </span><br><span class="line">         <span class="number">3</span>,<span class="number">7</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="built_in">input</span> = torch.Tensor(<span class="built_in">input</span>).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">conv_layer = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, bias=<span class="literal">False</span>)</span><br><span class="line">kernel = torch.Tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">conv_layer.weight.data = kernel.data</span><br><span class="line">output = conv_layer(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<h3 id="Max-Pooling-Layer"><a href="#Max-Pooling-Layer" class="headerlink" title="Max Pooling Layer"></a>Max Pooling Layer</h3><p>通道数量不变，池化层2 * 2默认是stride为2</p>
<p><img src="/2022/06/15/pytorch/image-20220706195007210.png" alt="image-20220706195007210"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">input</span> = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">5</span>, </span><br><span class="line">         <span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>, </span><br><span class="line">         <span class="number">1</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>, </span><br><span class="line">         <span class="number">9</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">6</span>, ]</span><br><span class="line"><span class="built_in">input</span> = torch.Tensor(<span class="built_in">input</span>).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">maxpooling_layer = torch.nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">output = maxpooling_layer(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="comment"># 注意maxpooling和激活层无区别，平均池化的话得先激活</span></span><br></pre></td></tr></table></figure>

<h3 id="实例一代码总结"><a href="#实例一代码总结" class="headerlink" title="实例一代码总结"></a>实例一代码总结</h3><p>现在我们再来看这个图，是不是格外清楚啦？</p>
<p><img src="/2022/06/15/pytorch/image-20220706195441686.png" alt="image-20220706195441686"></p>
<p>​        我们要保证一层一层输出，池化，他的维度一定是要能对上的，但其实，卷积层和池化层，它并不在意你的输入大小，因为设置好以后，无论多大的图像，它都能处理，但是，我们最后的分类器，就是通过我们这层层筛选出信息最后给予它的，所以也要好好分析怎么构建。</p>
<p>​        实际上我们也有偷懒的办法，不去算维度，直接print其实也不错hhhhh</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span> <span class="comment"># 99%高分</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">320</span>, <span class="number">10</span>) 		<span class="comment"># 用全连接层变换，所以最后一层不做激活</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="comment"># Flatten data from (n, 1, 28, 28) to (n, 784)</span></span><br><span class="line">    batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">    x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">    x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">    x = x.view(batch_size, -<span class="number">1</span>) <span class="comment"># flatten</span></span><br><span class="line">    x = self.fc(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">model = Net()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="comment"># 自己构建的另外一个模型，效果不是很好。。98%</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">12</span>, kernel_size=<span class="number">3</span>) <span class="comment"># 12 * 26 * 26</span></span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">12</span>, <span class="number">30</span>, kernel_size=<span class="number">4</span>) <span class="comment"># 30 * 10 * 10 </span></span><br><span class="line">        self.conv3 = torch.nn.Conv2d(<span class="number">30</span>, <span class="number">24</span>, kernel_size=<span class="number">2</span>) <span class="comment"># 24 * 4 * 4 </span></span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>) </span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">96</span>, <span class="number">10</span>) <span class="comment"># conv3后经过pooling 变成 24 * 2 * 2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = self.pooling(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pooling(F.relu(self.conv2(x)))</span><br><span class="line">        x = self.pooling(F.relu(self.conv3(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>) <span class="comment"># x传入的时候就是四维啦， batch-size channel width height</span></span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br><span class="line"></span><br><span class="line">model = Net()</span><br></pre></td></tr></table></figure>

<p><strong>完整代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader <span class="comment"># 注意大小写</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(), <span class="comment"># 将图像转化为图像张量（自动完成）pil image-&gt; tensor</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.1307</span>, ), (<span class="number">0.3081</span>, ))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>,</span><br><span class="line">                                train=<span class="literal">True</span>,</span><br><span class="line">                                download=<span class="literal">True</span>,</span><br><span class="line">                                transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset,</span><br><span class="line">                            shuffle=<span class="literal">True</span>,</span><br><span class="line">                            batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>,</span><br><span class="line">                                train=<span class="literal">False</span>,</span><br><span class="line">                                download=<span class="literal">True</span>,</span><br><span class="line">                                transform=transform)</span><br><span class="line">test_loader = DataLoader(train_dataset,</span><br><span class="line">                            shuffle=<span class="literal">False</span>,</span><br><span class="line">                            batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">320</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># Flatten data from (n, 1, 28, 28) to (n, 784)</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>) <span class="comment"># flatten</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">model = Net()</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum = <span class="number">0.5</span>) <span class="comment"># 这个可不能将就线性模型的用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        inputs, target = inputs.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># forward + backward + update</span></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class="number">1</span>, batch_idx + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            inputs, target = data</span><br><span class="line">            inputs, target = inputs.to(device), target.to(device)</span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += target.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == target).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy on test set: %d %% [%d/%d]&#x27;</span> % (<span class="number">100</span> * correct / total, correct, total))</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>: <span class="comment"># 是一个标志，类似象征着Java等语言中的程序主入口，告诉其他程序员，代码入口在此</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br></pre></td></tr></table></figure>

<p><img src="/2022/06/15/pytorch/QQ%E5%9B%BE%E7%89%8720220707111800.png" alt="QQ图片20220707111800"></p>
<h3 id="显卡运算"><a href="#显卡运算" class="headerlink" title="显卡运算"></a>显卡运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># cuda:0 cuda:1 代表你将使用那块显卡进行这个任务，看你机器多少块hhh</span></span><br><span class="line">model.to(device) <span class="comment"># 将模型的参数，缓存等等所有的模块都放到cuda里面</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 之前我们定义过train方法（对测试集也做同样处理）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    running_loss = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        <span class="comment"># inputs, target = inputs.to(device), target.to(device) # 一定要同一块GPU哦</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        output = model(inputs)</span><br><span class="line">        loss = criterion(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span>  == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch+<span class="number">1</span>, batch_idx+<span class="number">1</span>, running_loss/<span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.</span></span><br></pre></td></tr></table></figure>

<h2 id="高级CNN"><a href="#高级CNN" class="headerlink" title="高级CNN"></a>高级CNN</h2><p>​        我们在之前介绍的网络架构中，他们都是串行的结构，也就是每一层与每一层之间，依次构建输入输出关系，但神经网络往往会有更为复杂的结构，比如说可能会有分支，或者将此次的输出取出，用于别的作用。</p>
<h3 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h3><p><img src="/2022/06/15/pytorch/image-20220707115324565.png" alt="image-20220707115324565"></p>
<p>​        看到这种代码也许会手足无措，很明显如果我们要一一去定义的话，就太麻烦了，代码十分冗余，为此，我们先介绍一下减少代码冗余——-<code>函数，类</code></p>
<p>​        图中的Inception Module是如何实现的，就是我们要考虑的问题，观察一下其实发现存在大量重复的区域块。</p>
<p>​        在我们构建网络的过程中，选择哪一个卷积核尺寸比较好用，其实是比较难的，这也是googlenet的出发点，我们不知道那个尺寸比较好用，那我们就在一个块中把几个卷积都用一下，然后把结果摞到一起，将来如果3 * 3好用，自然而然，这个块的权重就比较大，比较重要了。</p>
<p>​        我们在构建的时候必须保证每一个小节点的宽度高度一致（b，c，w，h）唯一不同的就只能是cannel，保证高宽一致，做做padding就可以了</p>
<p><img src="/2022/06/15/pytorch/image-20220707143253506.png" alt="image-20220707143253506">        这里就给出多种方案，只要保证最后传出去的一样就行，AveragePooling，如果设置他stride和padding，其实他就是一个一维的卷积核，只不过权重是平均值而已。</p>
<p><strong>一维卷积运算过程</strong></p>
<p><img src="/2022/06/15/pytorch/image-20220707143431510.png" alt="image-20220707143431510"></p>
<p>​        这是一维卷积的计算过程，也是<strong>信息融合</strong>，类比于高中的科目，有多种科目，可能两个人均存在优势科目和弱势科目，导致分数有来有回，最简单的一个办法就是求总分来评判一个人的实力。</p>
<p>​        1 * 1卷积最重要的作用也在于此，就是改变通道的数量</p>
<p><img src="/2022/06/15/pytorch/image-20220707144449849.png" alt="image-20220707144449849"></p>
<p>​        从输入经过卷积层到输出的网络，计算次数是如何算出来的呢? 首先5 * 5，是一次卷积运算后得到的计算次数，其次因为我们要保证输入输出的w h一致，所以做了padding，也就是在一层中，我们要做28 * 28次卷积运算，然后我们总共有192个通道，所以说要 * 192，这样，我们能得到 28 * 28的单通道输出，由于输出需要32层，所以我们要做32次同样的运算，可以用相同的卷积层（可能），也可以用不同的卷积层，总之结果而言，就是要重复32次相同的操作，输出32层对应层，这算是人为设定的超参数。</p>
<p>​        注意观察加入了1 * 1卷积层后的网络，<u>以退为进</u>的思想，很显然它使得运算次数大大降低，只有<code>十分之一</code>的运算次数。</p>
<p><img src="/2022/06/15/pytorch/image-20220707145215534.png" alt="image-20220707145215534"></p>
<p>​        我们将每个分支的输出结果的cannel都叠加在一起，这个过程就称为<code>Concatenate</code></p>
<p><img src="/2022/06/15/pytorch/image-20220707145320151.png" alt="image-20220707145320151"></p>
<p>​        注意我们要沿着第一个维度拼接起来，所以维度为1，（b，c，w，h）</p>
<h4 id="代码实现-4"><a href="#代码实现-4" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将该代码替换前CNN初阶的Net即可使用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionA</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(InceptionA, self).__init__()</span><br><span class="line">        self.branch1x1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_1 = nn.Conv2d(in_channels,<span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.branch3x3_1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2 = nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_3 = nn.Conv2d(<span class="number">24</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch_pool = nn.Conv2d(in_channels, <span class="number">24</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line">        branch3x3 = self.branch3x3_3(branch3x3)</span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line">        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">1</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">88</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.incep1 = InceptionA(in_channels=<span class="number">10</span>)</span><br><span class="line">        self.incep2 = InceptionA(in_channels=<span class="number">20</span>)</span><br><span class="line">        self.mp = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">1408</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        in_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.mp(self.conv1(x)))</span><br><span class="line">        x = self.incep1(x)</span><br><span class="line">        x = F.relu(self.mp(self.conv2(x)))</span><br><span class="line">        x = self.incep2(x)</span><br><span class="line">        x = x.view(in_size, -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><blockquote>
<p>引言：如果把3 * 3的卷积一直堆叠下去，它的性能会不会变好呢？ 在CIFAR 10这数据集中，的确不会变好。</p>
</blockquote>
<p><img src="/2022/06/15/pytorch/image-20220707150346391.png" alt="image-20220707150346391"></p>
<p>​        引起这个现象的一个可能，叫做<code>梯度消失</code>，因为我们做的是反向传播，所以需要用链式法则，把一连串的梯度乘起来，假如我们每一处的梯度都是小于1的，把一连串的梯度乘起来，这个值就会越来越小越来越小最终趋于0了，当它趋于0的时候</p>
<img src="/2022/06/15/pytorch/image-20220707150709978.png" alt="image-20220707150709978" style="zoom:50%;">

<p>​        权重将会得不到什么更新，所以说在最开始，离输入比较近的那些块没办法得到充分的训练（反向传播，越乘回去越小）</p>
<p>所以以前为了解决梯度消失，提出了一些方法</p>
<p><img src="/2022/06/15/pytorch/image-20220707150956860.png" alt="image-20220707150956860"></p>
<p>​        用MNIST举例，这采用的是逐层训练，比如最后都是10个分类器，我们就每训练一层，就对它加一把锁，每次就训练一层，但是对于深度学习，这是非常困难的，因为层数太多了。</p>
<p><img src="/2022/06/15/pytorch/image-20220707151240381.png" alt="image-20220707151240381"></p>
<p>​        这就是Residual net，为什么说它能解决梯度消失的问题呢？</p>
<p>​        这里我们就用反向传播的一步来举例，假如说这一步的结果就是Z，反向传播后偏导为是σz / σx，因此每一步运算，都会在最初那个小于1的很小的值基础上加1，这样就保证了，即使数值真的很小，但他不会趋近于0，而是趋近于1，这样也能很好的训练到最开始的那些权重。</p>
<p>​        其中x直接连接到relu层那一步也有时被称为<code>跳连接</code>。对每一步的输入都做这样的操作，就能实现resNet</p>
<h4 id="MNIST-again"><a href="#MNIST-again" class="headerlink" title="MNIST again"></a>MNIST again</h4><p>整体过程的描述：</p>
<p><img src="/2022/06/15/pytorch/image-20220707152127861.png" alt="image-20220707152127861"></p>
<h4 id="代码实现-5"><a href="#代码实现-5" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResiualBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResiualBlock, self).__init__()</span><br><span class="line">        self.channel = channels</span><br><span class="line">        self.conv1 = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>) <span class="comment"># 3 * 3卷积层，padding（3/2）1层即可</span></span><br><span class="line">        self.conv2 = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        y = F.relu(self.conv1(x)) <span class="comment"># 激活与卷积更配哦，每卷一次一定要激活一次</span></span><br><span class="line">        y = self.conv2(y)</span><br><span class="line">        <span class="keyword">return</span> F.relu(x+y)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pooling = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        self.RBlock1 = ResiualBlock(<span class="number">16</span>)</span><br><span class="line">        self.RBlock2 = ResiualBlock(<span class="number">32</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        in_size = x.size(<span class="number">0</span>) <span class="comment"># 第0个维度的size，也就是batch-size</span></span><br><span class="line">        x = self.pooling(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.RBlock1(x)</span><br><span class="line">        x = self.pooling(F.relu(self.conv2(x)))</span><br><span class="line">        x = self.RBlock2(x)</span><br><span class="line">        x = x.view(in_size, -<span class="number">1</span>)   <span class="comment"># 要记得重塑</span></span><br><span class="line">        <span class="keyword">return</span> self.linear(x)</span><br></pre></td></tr></table></figure>

<h2 id="基本RNN"><a href="#基本RNN" class="headerlink" title="基本RNN"></a>基本RNN</h2><p>什么叫做RNN?</p>
<p>​        假设我们现在有一个序列，这里面x1，x2，x3，x4是我们的输入序列，比如说预测下一天天气，那么这个x就是这一天的某几个特征啥的，将x送到RNN里面，做一个线性变换（RNN其实就是线性变换），最后得到一个输出，叫做<code>hidden</code>，并且顺手可以把当前的h送到下一层的RNN中，对于第一项，如果h0没有值，那就设置成和之后的输出h同样维度的全0矩阵就行。对于每一个RNN线性层，我们可以设置为同样的权重，这样节省权重对内存的损耗使得同一个RNN层循环使用。</p>
<p><img src="/2022/06/15/pytorch/image-20220708144155758.png" alt="image-20220708144155758"></p>
<p><img src="/2022/06/15/pytorch/image-20220708144643804.png" alt="image-20220708144643804"></p>
<p>​        对于一层的RNN我们通常这样构建，（W<del>ih</del>其实写成W<del>hi</del>会更加容易理解）我们采用tanh作为激活函数是因为我们通常更加偏向于想让值保证在[-1, 1]之间。总之最后我们会通过一种构建矩阵的形式，使得它生成对应的h</p>
<p>​        pytorch里有函数可以帮助你构建</p>
<p><img src="/2022/06/15/pytorch/image-20220708145220251.png" alt="image-20220708145220251"></p>
<p>​        现在我们来举一个例</p>
<p><img src="/2022/06/15/pytorch/image-20220708145319027.png" alt="image-20220708145319027"></p>
<p>​        <code>batchSize</code>是一次取出多少个句子<code>seqLen</code>为句子的长度，也就是有三个词汇嘛，<code>inputsize</code>为每个词汇的特征个数，<code>hiddensize</code>为隐层的变换函数</p>
<p>​        要自己构建RNNCell层，就需要考虑这些，特别是维度</p>
<p><img src="/2022/06/15/pytorch/image-20220708150349305.png" alt="image-20220708150349305"></p>
<p>​        要构建RNN，先搞清楚每一个传入参数对应着什么。</p>
<p><img src="/2022/06/15/pytorch/image-20220708150624795.png" alt="image-20220708150624795"></p>
<p>​        需要使用RNN，仍然需要上述RNNCell的变量，同时还加了一个numLayers</p>
<p><img src="/2022/06/15/pytorch/image-20220708150937929.png" alt="image-20220708150937929"></p>
<p>​        那么什么是numLayers?</p>
<p><img src="/2022/06/15/pytorch/image-20220708151237811.png" alt="image-20220708151237811"></p>
<p>​        numLayers就是RNN的层数，例如这里有红，黄，绿三种线性变换层，就是有3层numLayers，本质上就是三种线性变换，然后不断循环就完事，只不过第一层的输出作为第二层的输入继续传，并且第一层的输出也作为第一层的后续迭代输入就绪传，看图就可知，左下为输入，右上为输出，整体式比较清晰的。</p>
<p><img src="/2022/06/15/pytorch/image-20220708151610596.png" alt="image-20220708151610596"></p>
<p>​        除此之外，RNN还有一些其他的参数，例如<code>batch_first</code>，它的作用就是交换输入数据集的两个维度，把batch_size放最前面方便与其他函数进行交互，例如将输出的out再接一层线性层，因为如果out维度batchsize在中间，线性变换肯定就是错的嘛</p>
<p><img src="/2022/06/15/pytorch/image-20220708151814943.png" alt="image-20220708151814943"></p>
<h3 id="RNN小任务"><a href="#RNN小任务" class="headerlink" title="RNN小任务"></a>RNN小任务</h3><p><img src="/2022/06/15/pytorch/image-20220708152148583.png" alt="image-20220708152148583"></p>
<p>​         首先我们会遇到一个问题，hello和ohlol并不是一个向量，我们往里面输入肯定会导致无法计算，所以我们要把字符向量化，用向量来表示它。</p>
<p>​        <strong>第一步</strong>，给每一个词，分配一个索引。</p>
<p>​        使用one-hot分配索引，可以消除索引之间隐形存在的大小关系，并且保证每个索引独一无二且方便矩阵运算。</p>
<p><img src="/2022/06/15/pytorch/image-20220708152541881.png" alt="image-20220708152541881"></p>
<p>一共有四个特殊的字符，所以input_size为4，seqlen为5</p>
<img src="/2022/06/15/pytorch/image-20220708152814519.png" alt="image-20220708152814519" style="zoom:50%;">

<p>此时，所有输入就变成一个一个的独热向量了。所以这个问题转变为一个多分类问题</p>
<p><img src="/2022/06/15/pytorch/image-20220708153249434.png" alt="image-20220708153249434">        这就是我们将来的输入和输出所需要的结构。</p>
<p><img src="/2022/06/15/pytorch/image-20220708153446914.png" alt="image-20220708153446914"></p>
<p>​        最后得出的结果，进行交叉熵损失即可得到loss</p>
<h4 id="代码实现-6"><a href="#代码实现-6" class="headerlink" title="代码实现"></a>代码实现</h4><h5 id="RNNCell实现"><a href="#RNNCell实现" class="headerlink" title="RNNCell实现"></a>RNNCell实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">input_size = <span class="number">4</span></span><br><span class="line">hidden_size = <span class="number">4</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">idx2char = [<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;o&#x27;</span>]</span><br><span class="line">x_data = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>] <span class="comment"># hello</span></span><br><span class="line">y_data = [<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>] <span class="comment"># ohlol</span></span><br><span class="line"></span><br><span class="line">one_hot_lookup = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                  [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]] <span class="comment"># 独热记录四词汇</span></span><br><span class="line"></span><br><span class="line">x_one_hot = [one_hot_lookup[x] <span class="keyword">for</span> x <span class="keyword">in</span> x_data] <span class="comment">#构建句子</span></span><br><span class="line"><span class="comment"># [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 0, 1]]</span></span><br><span class="line"></span><br><span class="line">inputs = torch.Tensor(x_one_hot).view(-<span class="number">1</span>, batch_size, input_size) <span class="comment"># 列表转变为tensor张量 5 * 1 * 4</span></span><br><span class="line">                                                                  <span class="comment"># 𝒔𝒆𝒒𝑳𝒆𝒏, 𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒉𝒊𝒅𝒅𝒆𝒏𝑺𝒊𝒛e</span></span><br><span class="line">labels = torch.LongTensor(y_data).view(-<span class="number">1</span>, <span class="number">1</span>) <span class="comment"># 将y_data转变为分类任务的标签，我们都知道，列数才是维度</span></span><br><span class="line"><span class="comment"># tensor([[3],                                # 𝒔𝒆𝒒𝑳𝒆𝒏 × 𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 1</span></span><br><span class="line">        <span class="comment"># [1],</span></span><br><span class="line">        <span class="comment"># [2],</span></span><br><span class="line">        <span class="comment"># [3],</span></span><br><span class="line">        <span class="comment"># [2]])</span></span><br><span class="line">inputs</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 1., 0., 0.]],

        [[1., 0., 0., 0.]],

        [[0., 0., 1., 0.]],

        [[0., 0., 1., 0.]],

        [[0., 0., 0., 1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, batch_size</span>):</span> <span class="comment"># 这里最开始写成了 B H I</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.rnncell = torch.nn.RNNCell(input_size=self.input_size,</span><br><span class="line">                                        hidden_size=self.hidden_size)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden</span>):</span></span><br><span class="line">        hidden = self.rnncell(<span class="built_in">input</span>, hidden) <span class="comment">#h0-&gt;h1</span></span><br><span class="line">        <span class="keyword">return</span> hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span>(<span class="params">self</span>):</span> <span class="comment"># 构建h0,工具类方法</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(self.batch_size, self.hidden_size)</span><br><span class="line"></span><br><span class="line">net = Model(input_size, hidden_size, batch_size)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">15</span>):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    hidden = net.init_hidden()               <span class="comment"># h0</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Predicted string: &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span>, label <span class="keyword">in</span> <span class="built_in">zip</span>(inputs, labels): <span class="comment"># 每次取出一组，例如第一组[0, 1, 0, 0] [3]</span></span><br><span class="line">        hidden = net(<span class="built_in">input</span>, hidden)          <span class="comment"># h1</span></span><br><span class="line">        <span class="comment"># print(label.dtype)</span></span><br><span class="line">        <span class="comment"># print(hidden.dtype)</span></span><br><span class="line">        loss += criterion(hidden, label)     <span class="comment"># 第一次的输出和正确答案label比对,注意不要.item()我们要构建图！</span></span><br><span class="line">                                             <span class="comment"># 我靠，大坑，loss必须要第一个为int64， 第二个为float32才能运行啊！！！</span></span><br><span class="line">        _, idx = hidden.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># print(hidden,&#x27;    &#x27;, label)</span></span><br><span class="line">        <span class="built_in">print</span>(idx2char[idx.item()], end=<span class="string">&#x27;&#x27;</span>)  <span class="comment"># 输出最大可能词汇。</span></span><br><span class="line">    </span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;, Epoch [%d/15] loss=%.4f&#x27;</span> % (epoch + <span class="number">1</span>, loss.item()))</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Predicted string: ooool, Epoch [<span class="number">1</span>/<span class="number">15</span>] loss=<span class="number">6.5547</span></span><br><span class="line">Predicted string: ooool, Epoch [<span class="number">2</span>/<span class="number">15</span>] loss=<span class="number">5.5869</span></span><br><span class="line">Predicted string: olool, Epoch [<span class="number">3</span>/<span class="number">15</span>] loss=<span class="number">4.9142</span></span><br><span class="line">Predicted string: ollll, Epoch [<span class="number">4</span>/<span class="number">15</span>] loss=<span class="number">4.4360</span></span><br><span class="line">Predicted string: oholl, Epoch [<span class="number">5</span>/<span class="number">15</span>] loss=<span class="number">4.0270</span></span><br><span class="line">Predicted string: ohool, Epoch [<span class="number">6</span>/<span class="number">15</span>] loss=<span class="number">3.7041</span></span><br><span class="line">Predicted string: ohlol, Epoch [<span class="number">7</span>/<span class="number">15</span>] loss=<span class="number">3.4379</span></span><br><span class="line">Predicted string: ohlol, Epoch [<span class="number">8</span>/<span class="number">15</span>] loss=<span class="number">3.1885</span></span><br><span class="line">Predicted string: ohlol, Epoch [<span class="number">9</span>/<span class="number">15</span>] loss=<span class="number">2.9624</span></span><br><span class="line">Predicted string: ohlol, Epoch [<span class="number">10</span>/<span class="number">15</span>] loss=<span class="number">2.7789</span></span><br><span class="line">Predicted string: ohlol, Epoch [<span class="number">11</span>/<span class="number">15</span>] loss=<span class="number">2.6578</span></span><br><span class="line">Predicted string: ohlol, Epoch [<span class="number">12</span>/<span class="number">15</span>] loss=<span class="number">2.5847</span></span><br><span class="line">Predicted string: ohlol, Epoch [<span class="number">13</span>/<span class="number">15</span>] loss=<span class="number">2.5270</span></span><br><span class="line">Predicted string: ohlol, Epoch [<span class="number">14</span>/<span class="number">15</span>] loss=<span class="number">2.4630</span></span><br><span class="line">Predicted string: ohlol, Epoch [<span class="number">15</span>/<span class="number">15</span>] loss=<span class="number">2.3770</span></span><br></pre></td></tr></table></figure>

<h4 id="改进模型"><a href="#改进模型" class="headerlink" title="改进模型"></a>改进模型</h4><h5 id="RNN实现"><a href="#RNN实现" class="headerlink" title="RNN实现"></a>RNN实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">input_size = <span class="number">4</span></span><br><span class="line">hidden_size = <span class="number">4</span></span><br><span class="line">num_layers = <span class="number">1</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">seq_len = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, batch_size, num_layers=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.rnn = torch.nn.RNN(input_size=self.input_size,</span><br><span class="line">                                hidden_size=self.hidden_size,</span><br><span class="line">                                num_layers=self.num_layers</span><br><span class="line">                                )<span class="comment"># 输出为(output,h_n)其中，output为Tensor(L, D * H_out）[未指定输入时]  </span></span><br><span class="line">                                 <span class="comment"># L:seqLen, H_out 为 hidden_size， D为1把就把他看成</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        hidden = torch.zeros(self.num_layers,</span><br><span class="line">                             self.batch_size,</span><br><span class="line">                             self.hidden_size)<span class="comment"># 𝒏𝒖𝒎𝑳𝒂𝒚𝒆𝒓𝒔, 𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒉𝒊𝒅𝒅𝒆𝒏𝑺𝒊𝒛e</span></span><br><span class="line">        out, _= self.rnn(<span class="built_in">input</span>, hidden)</span><br><span class="line">        <span class="keyword">return</span> out.view(-<span class="number">1</span>, self.hidden_size) <span class="comment"># 𝒔𝒆𝒒𝑳𝒆𝒏 × 𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒉𝒊𝒅𝒅𝒆𝒏𝑺𝒊𝒛e</span></span><br><span class="line"></span><br><span class="line">net = Model(input_size, hidden_size, batch_size, num_layers)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">0.05</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">labels = torch.LongTensor(y_data)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">15</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    outs = net(inputs)          <span class="comment"># h1</span></span><br><span class="line">    <span class="comment"># print(label.dtype)</span></span><br><span class="line">    <span class="comment"># print(hidden.dtype)</span></span><br><span class="line">    loss = criterion(outs, labels)        <span class="comment"># 第一次的输出和正确答案label比对,注意不要.item()我们要构建图！</span></span><br><span class="line">                                            <span class="comment"># 我靠，大坑，loss必须要第一个为int64， 第二个为float32才能运行啊！！！</span></span><br><span class="line">    _, idx = outs.<span class="built_in">max</span>(dim=<span class="number">1</span>)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    idx = idx.data.numpy()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;predicted:&#x27;</span>, <span class="string">&#x27;&#x27;</span>.join([idx2char[x] <span class="keyword">for</span> x <span class="keyword">in</span> idx]), end=<span class="string">&#x27;&#x27;</span>)  <span class="comment"># 输出最大可能词汇。</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;, Epoch [%d/15] loss=%.3f&#x27;</span> % (epoch + <span class="number">1</span>, loss.item()))</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">predicted: ohohh, Epoch [<span class="number">1</span>/<span class="number">15</span>] loss=<span class="number">1.366</span></span><br><span class="line">predicted: ohooh, Epoch [<span class="number">2</span>/<span class="number">15</span>] loss=<span class="number">1.208</span></span><br><span class="line">predicted: ohooh, Epoch [<span class="number">3</span>/<span class="number">15</span>] loss=<span class="number">1.081</span></span><br><span class="line">predicted: ohool, Epoch [<span class="number">4</span>/<span class="number">15</span>] loss=<span class="number">0.990</span></span><br><span class="line">predicted: ohool, Epoch [<span class="number">5</span>/<span class="number">15</span>] loss=<span class="number">0.924</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">6</span>/<span class="number">15</span>] loss=<span class="number">0.872</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">7</span>/<span class="number">15</span>] loss=<span class="number">0.829</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">8</span>/<span class="number">15</span>] loss=<span class="number">0.788</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">9</span>/<span class="number">15</span>] loss=<span class="number">0.750</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">10</span>/<span class="number">15</span>] loss=<span class="number">0.714</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">11</span>/<span class="number">15</span>] loss=<span class="number">0.680</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">12</span>/<span class="number">15</span>] loss=<span class="number">0.649</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">13</span>/<span class="number">15</span>] loss=<span class="number">0.621</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">14</span>/<span class="number">15</span>] loss=<span class="number">0.596</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">15</span>/<span class="number">15</span>] loss=<span class="number">0.572</span></span><br></pre></td></tr></table></figure>

<h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><blockquote>
<p>Using embedding and linear layer<br>这次我们来使用Embedding代替独热编码</p>
</blockquote>
<p>独热向量虽然能解决问题，也是有缺点的</p>
<ul>
<li><p>维度太高，每一个不同的就要加一维，面临维度的诅咒</p>
</li>
<li><p>向量过于稀疏，有数值的可能都集中在坐标轴上</p>
</li>
<li><p>硬编码，每个字符集对应的那个向量是人为指定的硬编码</p>
</li>
</ul>
<p>​        因此我们想要构建一个低维，稠密，可学习的变换</p>
<p>这就引出了大名鼎鼎的<code>Embedding</code></p>
<p><img src="/2022/06/15/pytorch/image-20220708155032463.png" alt="image-20220708155032463"></p>
<p>注意：这里数据准备这样写会报错，要改为input=torch.LongTensor(x_data).view(batch_size,seq_len)</p>
<h4 id="最后实现"><a href="#最后实现" class="headerlink" title="最后实现"></a>最后实现</h4><p>代替独热再次实现小任务</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># parameters</span></span><br><span class="line">num_class = <span class="number">4</span></span><br><span class="line">input_size = <span class="number">4</span></span><br><span class="line">hidden_size = <span class="number">8</span></span><br><span class="line">embedding_size = <span class="number">10</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">seq_len = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.emb = torch.nn.Embedding(input_size, embedding_size) <span class="comment"># (𝒊𝒏𝒑𝒖𝒕𝑺𝒊𝒛𝒆, 𝒆𝒎𝒃𝒆𝒅𝒅𝒊𝒏𝒈𝑺𝒊𝒛e) </span></span><br><span class="line">                                                                  <span class="comment"># Embedding，用于添加特征，扩维度，第一个参数为input_size,第二个参数为想要对每一个单词扩充的维度</span></span><br><span class="line">                                                                  <span class="comment"># 也就类似于给单词加特征，让单词更佳独特</span></span><br><span class="line">        self.rnn = torch.nn.RNN(input_size=embedding_size,</span><br><span class="line">                                hidden_size=hidden_size,</span><br><span class="line">                                num_layers=num_layers,</span><br><span class="line">                                batch_first=<span class="literal">True</span>  <span class="comment"># batch_size维度放到最前面，方便后面接线性</span></span><br><span class="line">                                )</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_size, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        hidden = torch.zeros(num_layers, x.size(<span class="number">0</span>), hidden_size) <span class="comment"># x.size()为1，5，说明总共有两个维度，最外维有1个元素（size(0))，即[1, 0, 2, 2, 3]</span></span><br><span class="line">                                                                 <span class="comment"># 最内层有五个元素，即1, 0, 2, 2, 3。换言之，取出了batch_size，没有取出seqlen</span></span><br><span class="line">        x = self.emb(x) <span class="comment"># (batch, seqlen, embeddingsize)</span></span><br><span class="line">        x, _ = self.rnn(x, hidden)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>, num_class)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">idx2char = [<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;o&#x27;</span>]</span><br><span class="line">x_data = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]] <span class="comment"># (batch, seqlen) 仅这一个句子，也就是一个训练集</span></span><br><span class="line">y_data = [<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>] <span class="comment"># (batch, seqlen) 标准answer</span></span><br><span class="line"></span><br><span class="line">inputs = torch.LongTensor(x_data)</span><br><span class="line">labels = torch.LongTensor(y_data)</span><br><span class="line"></span><br><span class="line">net = Model()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">0.05</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">15</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    outputs = net(inputs)</span><br><span class="line">    loss = criterion(outputs, labels) <span class="comment">#构图，不用item取。要一组连续的，才是真的loss</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    _, idx = outputs.<span class="built_in">max</span>(dim=<span class="number">1</span>) <span class="comment"># num_class中最大</span></span><br><span class="line">    idx = idx.data.numpy()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;predicted:&#x27;</span>, <span class="string">&#x27;&#x27;</span>.join([idx2char[x] <span class="keyword">for</span> x <span class="keyword">in</span> idx]), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;, Epoch [%d/15] loss = %.3f&#x27;</span> % (epoch, loss.item()))</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">predicted: eeell, Epoch [<span class="number">0</span>/<span class="number">15</span>] loss = <span class="number">1.400</span></span><br><span class="line">predicted: ollll, Epoch [<span class="number">1</span>/<span class="number">15</span>] loss = <span class="number">1.104</span></span><br><span class="line">predicted: ohlll, Epoch [<span class="number">2</span>/<span class="number">15</span>] loss = <span class="number">0.891</span></span><br><span class="line">predicted: ohlll, Epoch [<span class="number">3</span>/<span class="number">15</span>] loss = <span class="number">0.682</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">4</span>/<span class="number">15</span>] loss = <span class="number">0.503</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">5</span>/<span class="number">15</span>] loss = <span class="number">0.362</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">6</span>/<span class="number">15</span>] loss = <span class="number">0.250</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">7</span>/<span class="number">15</span>] loss = <span class="number">0.168</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">8</span>/<span class="number">15</span>] loss = <span class="number">0.117</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">9</span>/<span class="number">15</span>] loss = <span class="number">0.083</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">10</span>/<span class="number">15</span>] loss = <span class="number">0.060</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">11</span>/<span class="number">15</span>] loss = <span class="number">0.044</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">12</span>/<span class="number">15</span>] loss = <span class="number">0.033</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">13</span>/<span class="number">15</span>] loss = <span class="number">0.025</span></span><br><span class="line">predicted: ohlol, Epoch [<span class="number">14</span>/<span class="number">15</span>] loss = <span class="number">0.019</span></span><br></pre></td></tr></table></figure>

<p>可以看到，达到了前所未有的效果！</p>
<h2 id="高级RNN"><a href="#高级RNN" class="headerlink" title="高级RNN"></a>高级RNN</h2><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>​        我们从一个简单的任务，引入语言模型<img src="/2022/06/15/pytorch/image-20220709145629825.png" alt="image-20220709145629825"></p>
<p>​        这个任务提供18种类别的国家，然后通过名字，判断这个人是来自于哪个国家的，</p>
<p>也就是说，它的输出只有<strong>一个</strong><img src="/2022/06/15/pytorch/image-20220709145809792.png" alt="image-20220709145809792"></p>
<p>​        上一讲里，我们的RNNCell，就是需要o1，o2，o3，o4，o5等多个输出。但是现在我们只需要一个，因此，其实模型不需要这么麻烦。</p>
<p><img src="/2022/06/15/pytorch/image-20220709150644089.png" alt="image-20220709150644089"></p>
<p>​        我们只需将最后一个输出取出，并将它做线性变换，使之映射到18个种类即可。</p>
<p>​        不过请注意，例如Name，随便找一个，比如Maclean，他是一个句子，seqlen是为7，’M’,’a’,’c’,’l’,’e’,’a’,’n’分别是x1，x2，x3，x4…的词汇哈，其实我们词汇就52个字母吧差不多（大小写）。</p>
<h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据<img src="/2022/06/15/pytorch/image-20220709151129548.png" alt="image-20220709151129548"></h4><p>​        首先我们需要把每一个名字构成一个序列，把数据拆分成一个又一个的词汇，为了要构建词典，我们选用ASCII表来进行每一个字母，但值得注意的是，这里的ASCII表的数字，其实就代表了独热编码的，第几行几列，它的值为1，剩下的就交给Embedding层就好了。</p>
<p>​        另外一个问题，这些序列是长短不一的，因此我们做一个padding，因为将来我们把它构成一个批量（当然，我们就训练单个就不必，这是为了保证训练集的整齐），进行运算，seqlen和batchsize，input得一样。因为<strong>张量需要保证所有的维度数据得填满，是一个完整的矩阵</strong></p>
<p><img src="/2022/06/15/pytorch/image-20220709151811256.png" alt="image-20220709151811256"></p>
<p><img src="/2022/06/15/pytorch/image-20220709151853356.png" alt="image-20220709151853356"></p>
<p>​        对于国家数据集，我们将每个数据集填上一个索引，就保证能进行查找了。</p>
<h4 id="双向循环网络"><a href="#双向循环网络" class="headerlink" title="双向循环网络"></a>双向循环网络</h4><p><img src="/2022/06/15/pytorch/image-20220709153447309.png" alt="image-20220709153447309"></p>
<p>​        双向首先，一定要和神经网络的forward，backward区分开，二者并不相同，他真的就是单纯的反向再走一次，第一个是从x1+h^f^<del>0</del>经过RNN Cell的线性变换后得到权重，在与X2进行结合，最终不断传播到Xn，得到最后的隐藏层h^f^<del>N</del>,而另外一个就是从Xn+h^b^<del>0</del>开始同样进行相同的RNN Cell线性变换得到权重，逐渐运算，最终得到h^b^<del>n</del>然后将同一个词汇Xn的两向计算结果concat，得到最终应该输出的隐藏层</p>
<p><img src="/2022/06/15/pytorch/image-20220709154926002.png" alt="image-20220709154926002"></p>
<p>​        hidden只有这两个。所以GRU最终输出的隐层会有两个</p>
<p>对于GRU，有一个提升效率的工具，叫做pack_padded_sequence,他能将数据进行打包，就是使得padding填充的无用数据不进行计算，每次只使用实际有效的数据，但他有个怪癖，他必须只能要降序的数据，也就是说我们在打包之前，应该对Embedding后的数据集进行降序排序，并且把每一个seqlen做成一个列表传给他，具体打包过程见下图。</p>
<blockquote>
<p>注意，每一纵行的数据也是从大到小排序的, 别被误导了</p>
</blockquote>
<p><img src="/2022/06/15/pytorch/image-20220709160426817.png" alt="image-20220709160426817"></p>
<h4 id="代码实现-7"><a href="#代码实现-7" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preparing Data</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NameDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, is_train_set=<span class="literal">True</span></span>):</span> <span class="comment"># is_train_set为人为设置的变量</span></span><br><span class="line">        <span class="built_in">super</span>(NameDataset, self).__init__()</span><br><span class="line">        filename = <span class="string">&#x27;names_train.csv.gz&#x27;</span> <span class="keyword">if</span> is_train_set <span class="keyword">else</span> <span class="string">&#x27;names_test.csv.gz&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> gzip.<span class="built_in">open</span>(filename, <span class="string">&#x27;rt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            reader = csv.reader(f)</span><br><span class="line">            rows = <span class="built_in">list</span>(reader)</span><br><span class="line">        self.names = [row[<span class="number">0</span>] <span class="keyword">for</span> row <span class="keyword">in</span> rows] <span class="comment"># row[0]为names， row[1]为countries</span></span><br><span class="line">        self.<span class="built_in">len</span> = <span class="built_in">len</span>(self.names)</span><br><span class="line">        self.countries = [row[<span class="number">1</span>] <span class="keyword">for</span> row <span class="keyword">in</span> rows]</span><br><span class="line">        self.country_list = <span class="built_in">list</span>(<span class="built_in">sorted</span>(<span class="built_in">set</span>(self.countries))) <span class="comment"># set清除多余country，保证每个独一无二，sort默认顺序排序，后转变为列表</span></span><br><span class="line">        self.country_dict = self.getCountryDict()  <span class="comment"># 一个小方法，得到字典</span></span><br><span class="line">        self.country_num = <span class="built_in">len</span>(self.country_list)  <span class="comment"># 国家的数目</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.names[index], self.country_dict[self.countries[index]] </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span> <span class="comment"># 数据集的大小（names的个数）</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getCountryDict</span>(<span class="params">self</span>):</span></span><br><span class="line">        country_dict = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> idx, country_name <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.country_list, <span class="number">0</span>): <span class="comment"># 对每个国家，将排序的对应序号赋给对应key的value</span></span><br><span class="line">            country_dict[country_name] = idx</span><br><span class="line">        <span class="keyword">return</span> country_dict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">idx2country</span> (<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.country_list[index] <span class="comment"># 注意这是list，取出对应索引的country</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getCountriesNum</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.country_num</span><br><span class="line">        </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">100</span></span><br><span class="line">BATCH_SIZE = <span class="number">256</span></span><br><span class="line">N_LAYER = <span class="number">2</span></span><br><span class="line">N_EPOCHS = <span class="number">100</span></span><br><span class="line">N_CHARS = <span class="number">128</span></span><br><span class="line">USE_GPU = <span class="literal">False</span> <span class="comment"># 有显卡就True咯</span></span><br><span class="line"></span><br><span class="line">trainset = NameDataset(is_train_set=<span class="literal">True</span>)</span><br><span class="line">trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line">testset = NameDataset(is_train_set=<span class="literal">False</span>)</span><br><span class="line">testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># the output size of our model</span></span><br><span class="line">N_COUNTRY = trainset.getCountriesNum()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name2list</span>(<span class="params">name</span>):</span></span><br><span class="line">    arr = [<span class="built_in">ord</span>(c) <span class="keyword">for</span> c <span class="keyword">in</span> name] <span class="comment"># 转码ASCII</span></span><br><span class="line">    <span class="keyword">return</span> arr, <span class="built_in">len</span>(arr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_tensors</span>(<span class="params">names, countries</span>):</span></span><br><span class="line">    sequences_and_lengths = [name2list(name) <span class="keyword">for</span> name <span class="keyword">in</span> names]   <span class="comment"># 对每一个姓名进行转码， 返回列表本身和列表长度</span></span><br><span class="line">    name_sequences = [s1[<span class="number">0</span>] <span class="keyword">for</span> s1 <span class="keyword">in</span> sequences_and_lengths]      <span class="comment"># 取出ASCII的一个句子（名字）一个数组列表</span></span><br><span class="line">    <span class="comment"># print(&#x27;make_tensors\&#x27;s name_sequences  &#x27;, name_sequences)</span></span><br><span class="line">    seq_lengths = torch.LongTensor([s1[<span class="number">1</span>] <span class="keyword">for</span> s1 <span class="keyword">in</span> sequences_and_lengths])     <span class="comment"># 取出该名字的长度，并转换为float32  </span></span><br><span class="line">    countries = countries.long()</span><br><span class="line">    <span class="comment"># print(countries)  # 输出全是2，那么确定是数据集的问题了</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># make tensor of name， BatchSize * Seqlen</span></span><br><span class="line">    seq_tensor = torch.zeros(<span class="built_in">len</span>(name_sequences), seq_lengths.<span class="built_in">max</span>()).long()     <span class="comment"># padding过程</span></span><br><span class="line">    <span class="keyword">for</span> idx, (seq, seq_len) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(name_sequences, seq_lengths), <span class="number">0</span>):  <span class="comment"># 创建一个0矩阵，再把我们已有的值贴过去</span></span><br><span class="line">        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)                       <span class="comment"># 名字的ASCII给这个点位，idx应该是enumerate自带变量</span></span><br><span class="line">                                                                                <span class="comment"># enumerate本身就是给对应元素加一个索引返回，这是从0开始</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># sort by length to use pack_padded_sequence </span></span><br><span class="line">    seq_lengths, perm_idx = seq_lengths.sort(dim=<span class="number">0</span>, descending=<span class="literal">True</span>) <span class="comment"># 0为纵，1为横</span></span><br><span class="line">    seq_tensor = seq_tensor[perm_idx]                                <span class="comment"># pytorch的sort返回排完序之后的序列，和此时对应的索引</span></span><br><span class="line">    countries = countries[perm_idx]                                  <span class="comment"># 好像用这个索引就能自动把tensor和country也排好序？</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> create_tensor(seq_tensor), \</span><br><span class="line">           create_tensor(seq_lengths), \</span><br><span class="line">           create_tensor(countries)  <span class="comment"># create_tensor判断是否使用GPU</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tensor</span>(<span class="params">tensor</span>):</span></span><br><span class="line">    <span class="keyword">if</span> USE_GPU:</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        tensor = tensor.to(device)</span><br><span class="line">        <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNClassifier</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, n_layers=<span class="number">1</span>, bidirectional=<span class="literal">True</span></span>):</span> <span class="comment"># bidirectional为是否双向</span></span><br><span class="line">        <span class="built_in">super</span>(RNNClassifier, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.n_layers = n_layers                <span class="comment"># layers&amp;&amp;hidden_size = GRU layer</span></span><br><span class="line">        self.n_directions = <span class="number">2</span> <span class="keyword">if</span> bidirectional <span class="keyword">else</span> <span class="number">1</span> <span class="comment">#单向还是双向, directions为方向</span></span><br><span class="line"></span><br><span class="line">        self.embedding = torch.nn.Embedding(input_size, hidden_size) <span class="comment"># input_size --&gt; (seqlen, batchsize)</span></span><br><span class="line">                                                                     <span class="comment"># output of Embedding --&gt; (seqlen, batchsize, hiddenSize)</span></span><br><span class="line">        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers, <span class="comment"># input output cannel </span></span><br><span class="line">                                bidirectional=bidirectional)    <span class="comment"># direction</span></span><br><span class="line">        self.fc = torch.nn.Linear(hidden_size*self.n_directions, output_size)  <span class="comment"># 将GRU输出结果，转换为output_size的尺寸</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_hidden</span>(<span class="params">self, batch_size</span>):</span></span><br><span class="line">        hidden = torch.zeros(self.n_layers*self.n_directions, <span class="comment"># hidden用作线性变换，n_layers*n_direction就是另外一个视角</span></span><br><span class="line">                            batch_size, self.hidden_size)     <span class="comment"># 看他是那根线的hidden</span></span><br><span class="line">        <span class="keyword">return</span> create_tensor(hidden)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, seq_lengths</span>):</span></span><br><span class="line">        <span class="comment"># input shape: B * S --&gt; S * B</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">input</span>)      <span class="comment"># 最开始是None，正在找哪里写错了，估计是数据集的问题</span></span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.t() <span class="comment"># Transpose</span></span><br><span class="line">        batch_size = <span class="built_in">input</span>.size(<span class="number">1</span>) <span class="comment"># 第二个维度的size取出（取出B中的东西）</span></span><br><span class="line"></span><br><span class="line">        hidden = self._init_hidden(batch_size) <span class="comment"># Save batch-size for make initial hidden.</span></span><br><span class="line">        embedding = self.embedding(<span class="built_in">input</span>)      <span class="comment"># Seqlen Batch Hidden</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack them up </span></span><br><span class="line">        gru_input = pack_padded_sequence(embedding, seq_lengths)  <span class="comment"># Result of embedding with shape </span></span><br><span class="line">                                                                  <span class="comment"># (𝑠𝑒𝑞𝐿𝑒𝑛, 𝑏𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒, ℎ𝑖𝑑𝑑𝑒𝑛𝑆𝑖𝑧e)</span></span><br><span class="line"></span><br><span class="line">        output, hidden = self.gru(gru_input, hidden)              <span class="comment"># hidden --&gt; (nLayers * nDirection, batchsize, hiddensize)</span></span><br><span class="line">        <span class="keyword">if</span> self.n_directions == <span class="number">2</span>:                                <span class="comment"># 如果存在双向，那么进行cat求和hidden</span></span><br><span class="line">            hidden_cat = torch.cat([hidden[-<span class="number">1</span>], hidden[-<span class="number">2</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hidden_cat = hidden[-<span class="number">1</span>]</span><br><span class="line">        fc_output = self.fc(hidden_cat)</span><br><span class="line">        <span class="keyword">return</span> fc_output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainModel</span>():</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, (names, countries) <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># print(countries) #countries很明显有问题</span></span><br><span class="line">        inputs, seq_lengths, target = make_tensors(names, countries)</span><br><span class="line">        <span class="comment"># print(inputs)   # None，继续追溯make_tensors</span></span><br><span class="line">        output = classifier(inputs, seq_lengths)</span><br><span class="line">        loss = criterion(output, target)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;[<span class="subst">&#123;time_since(start)&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;[<span class="subst">&#123;i * <span class="built_in">len</span>(inputs)&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(trainset)&#125;</span>] &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;loss=<span class="subst">&#123;total_loss / (i * <span class="built_in">len</span>(inputs))&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br><span class="line">        </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testModel</span>():</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="built_in">len</span>(testset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;evaluting trained model ...&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (names, countries) <span class="keyword">in</span> <span class="built_in">enumerate</span>(testloader, <span class="number">1</span>):</span><br><span class="line">            inputs, seq_lengths, target = make_tensors(names, countries)</span><br><span class="line">            output = classifier(inputs, seq_lengths)</span><br><span class="line">            pred = output.<span class="built_in">max</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line">            correct += pred.eq(target.view_as(pred)).<span class="built_in">sum</span>().item()<span class="comment"># operator.eq()函数是一个库函数operator模块，用于对两个值执行“等于操作”，如果第一个值等于第二个值返回True</span></span><br><span class="line">            												<span class="comment"># view_as返回被视作与给定的tensor相同大小的原tensor。 等效于，这里target的view与pred一致</span></span><br><span class="line"></span><br><span class="line">        percent = <span class="string">&#x27;%.2f&#x27;</span> % (<span class="number">100</span> * correct / total)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Test set: Accuracy <span class="subst">&#123;correct&#125;</span>/<span class="subst">&#123;total&#125;</span> <span class="subst">&#123;percent&#125;</span>%&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> correct / total</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_since</span>(<span class="params">since</span>):</span></span><br><span class="line">    s = time.time() - since</span><br><span class="line">    m = math.floor(s / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;%dm %ds&#x27;</span> % (m, s)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)</span><br><span class="line">    <span class="keyword">if</span> USE_GPU:</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        classifier.to(device)</span><br><span class="line">    criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(classifier.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training for %d epochs...&quot;</span> % N_EPOCHS)</span><br><span class="line">    acc_list = []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N_EPOCHS + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Train cycle</span></span><br><span class="line">        trainModel()</span><br><span class="line">        acc = testModel()</span><br><span class="line">        acc_list.append(acc)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    epoch = np.arange(<span class="number">1</span>, <span class="built_in">len</span>(acc_list) + <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    acc_list = np.array(acc_list)</span><br><span class="line">    plt.plot(epoch, acc_list)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Training for 100 epochs...


AttributeError: &#39;NoneType&#39; object has no attribute &#39;t&#39; # 最后这里报了个错，百思不得其解，但前面的意思已经差不多理解了。
</code></pre>
<h3 id="做古诗（了解）"><a href="#做古诗（了解）" class="headerlink" title="做古诗（了解）"></a>做古诗（了解）</h3><p><img src="/2022/06/15/pytorch/image-20220709191310132.png" alt="image-20220709191310132"></p>
<p><img src="/2022/06/15/pytorch/image-20220709191419876.png" alt="image-20220709191419876"></p>
<p>​        给出入会申请的标题，生成一大堆文字</p>
<p><img src="/2022/06/15/pytorch/image-20220709191518537.png" alt="image-20220709191518537"></p>
<p>但我们又不希望每次生成都一样，引入一种机制</p>
<h4 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h4><img src="/2022/06/15/pytorch/image-20220709191631113.png" alt="image-20220709191631113" style="zoom: 33%;">

<p>​        为此我们可以引入重要性采样，也就是不以谁的概率大就输出谁，而是说，谁的概率大，那么就有更大的机会抽中它，类似于一共六个人抽签，有五个人都只有一个球代表它自己，有一个人同时有两个球代表他，将七个球一起放到池子中，那么有两个球的同学会更容易被抽到，但的确是未必被抽到的。</p>
<h1 id="尾言"><a href="#尾言" class="headerlink" title="尾言"></a>尾言</h1><p>​        RNN主要用于解决各种各样序列问题，CNN主要用于解决平面，空间的，带有二维以及以上的序列图像，这个时候我们就考虑用CNN。    </p>
<p>​        由于神经网络这几年发展的很快，还会有各种各样的变体，比如RNN引入Attention，神经网络+传统概率模型近似的新模型，input为Graph，在Graph上进行信息融合。</p>
<p>​        不管是什么样的神经网络，最终的套路是什么呢？<strong>它肯定是一个数值型的输入变成我们想要的输出</strong></p>
<p>也就是：我们需要把原始的数据形式，转变为能够进行网络运算的数据，不管是CNN，RNN，GNN还是什么，都是再找的非线性空间变换，只要把空间变换做好，将来得到我们想要的输出就可以了。所以我们制定网络结构，先要满足前几个条件，然后我们可以加入各种各样改进的办法，提高模型的性能。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:jjyaoao@126.com">jjyaoao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://jjyaoao.space/2022/06/15/pytorch/">http://jjyaoao.space/2022/06/15/pytorch/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://jjyaoao.space" target="_blank">jjyaoao's Home</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%81%97%E5%BF%98%E6%8C%87%E5%8D%97/">遗忘指南</a><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-61dbf69078ce19aa" async="async"></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://s2.loli.net/2022/01/09/HMjygFAKr2nPWdJ.jpg" target="_blank"><img class="post-qr-code-img" src="https://s2.loli.net/2022/01/09/HMjygFAKr2nPWdJ.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://s2.loli.net/2022/01/09/JL43sbhxUEOuVeM.jpg" target="_blank"><img class="post-qr-code-img" src="https://s2.loli.net/2022/01/09/JL43sbhxUEOuVeM.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/07/02/%E7%BB%83%E8%BD%A6%E6%97%A5%E8%AE%B0/"><img class="prev-cover" src="https://s2.loli.net/2022/04/15/YXoFfbxgWZIn1SM.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">练车日记</div></div></a></div><div class="next-post pull-right"><a href="/2022/05/30/MySQL%E4%BD%BF%E7%94%A8/"><img class="next-cover" src="https://s2.loli.net/2022/04/15/YXoFfbxgWZIn1SM.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">MySQL使用</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/04/15/C++%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/" title="C++项目实战"><img class="cover" src="https://s2.loli.net/2022/04/15/YXoFfbxgWZIn1SM.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-15</div><div class="title">C++项目实战</div></div></a></div><div><a href="/2022/08/17/22%E5%B9%B4%E5%A4%87%E6%88%98%E6%95%B0%E6%A8%A1%E5%9B%BD%E8%B5%9B/" title="2022备战国赛"><img class="cover" src="https://s2.loli.net/2022/04/15/YXoFfbxgWZIn1SM.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-17</div><div class="title">2022备战国赛</div></div></a></div><div><a href="/2022/09/15/Cmake+Vscode+C++%E5%BC%80%E5%8F%91/" title="Cmake+vscode+C++"><img class="cover" src="https://s2.loli.net/2022/04/15/YXoFfbxgWZIn1SM.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-15</div><div class="title">Cmake+vscode+C++</div></div></a></div><div><a href="/2022/09/10/Git/" title="Git"><img class="cover" src="https://s2.loli.net/2022/04/15/YXoFfbxgWZIn1SM.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-10</div><div class="title">Git</div></div></a></div><div><a href="/2022/08/23/Geatpy-python%E4%BC%98%E5%8C%96%E5%BA%93/" title="Geatpy-python优化库"><img class="cover" src="https://s2.loli.net/2022/04/15/YXoFfbxgWZIn1SM.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-23</div><div class="title">Geatpy-python优化库</div></div></a></div><div><a href="/2022/04/15/Linux%E5%AD%A6%E4%B9%A0/" title="Linux学习"><img class="cover" src="https://s2.loli.net/2022/04/15/FltTRVUfbzYIowQ.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-15</div><div class="title">Linux学习</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Livere</span><span class="switch-btn"></span><span class="second-comment">Valine</span></div></div><div class="comment-wrap"><div><div id="lv-container" data-id="city" data-uid="MTAyMC81NTM1OC8zMTgyNQ=="></div></div><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">1.</span> <span class="toc-text">环境配置</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E7%82%B9%E7%82%B9%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF"><span class="toc-number">2.</span> <span class="toc-text">一点点学习路线</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5"><span class="toc-number">3.</span> <span class="toc-text">学习实践</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">线性模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">梯度下降法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">3.2.1.</span> <span class="toc-text">随机梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">3.2.2.</span> <span class="toc-text">批量随机梯度下降</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.3.</span> <span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC%E6%B3%95%E5%88%99"><span class="toc-number">3.3.1.</span> <span class="toc-text">链式求导法则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E5%85%A5Tensor"><span class="toc-number">3.3.2.</span> <span class="toc-text">引入Tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#backward-%E5%8F%8A%E5%85%B6%E4%BB%96%E5%AE%9E%E7%94%A8fuc"><span class="toc-number">3.3.3.</span> <span class="toc-text">backward()及其他实用fuc</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">3.4.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#pytorch%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6"><span class="toc-number">3.4.1.</span> <span class="toc-text">pytorch模型框架</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.4.1.1.</span> <span class="toc-text">构建数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.4.1.2.</span> <span class="toc-text">定义模型</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#linear-%E5%8F%82%E6%95%B0%E8%A7%A3%E9%87%8A"><span class="toc-number">3.4.1.2.1.</span> <span class="toc-text">linear()参数解释</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#call-%E6%96%B9%E6%B3%95%E4%B8%8E%E8%87%AA%E5%AE%9A%E4%BC%A0%E5%8F%82"><span class="toc-number">3.4.1.2.2.</span> <span class="toc-text">call()方法与自定传参</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E8%BF%AD%E4%BB%A3%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.4.1.3.</span> <span class="toc-text">构建迭代器和损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">3.4.1.4.</span> <span class="toc-text">训练过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%93%E5%8D%B0%E7%BB%93%E6%9E%9C"><span class="toc-number">3.4.1.5.</span> <span class="toc-text">打印结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.4.1.6.</span> <span class="toc-text">代码实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic%E5%9B%9E%E5%BD%92"><span class="toc-number">3.5.</span> <span class="toc-text">Logistic回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.5.1.</span> <span class="toc-text">数据集介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#logistic%E5%87%BD%E6%95%B0"><span class="toc-number">3.5.2.</span> <span class="toc-text">logistic函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A5%B1%E5%92%8C%E5%87%BD%E6%95%B0"><span class="toc-number">3.5.3.</span> <span class="toc-text">饱和函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94"><span class="toc-number">3.5.4.</span> <span class="toc-text">与线性模型对比</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E7%A8%8B%E5%BC%8F"><span class="toc-number">3.5.4.1.</span> <span class="toc-text">方程式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.5.4.2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AF%B9%E6%AF%94"><span class="toc-number">3.5.4.3.</span> <span class="toc-text">代码对比</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%BB%B4%E8%BE%93%E5%85%A5"><span class="toc-number">3.5.5.</span> <span class="toc-text">多维输入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E8%A7%A3%E9%87%8A"><span class="toc-number">3.5.5.1.</span> <span class="toc-text">数据集解释</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B8%85%E6%99%B0%E7%9A%84%E6%8E%A8%E5%AF%BC"><span class="toc-number">3.5.5.2.</span> <span class="toc-text">清晰的推导</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4%E5%A4%84%E7%90%86"><span class="toc-number">3.5.5.3.</span> <span class="toc-text">降维处理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.6.</span> <span class="toc-text">加载数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5%E8%AF%B4%E6%98%8E"><span class="toc-number">3.6.1.</span> <span class="toc-text">概念说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">3.6.2.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="toc-number">3.7.</span> <span class="toc-text">多分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax%E5%87%BD%E6%95%B0"><span class="toc-number">3.7.1.</span> <span class="toc-text">softmax函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NLLLoss"><span class="toc-number">3.7.2.</span> <span class="toc-text">NLLLoss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MNIST-Solution"><span class="toc-number">3.7.3.</span> <span class="toc-text">MNIST Solution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">3.7.4.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E5%88%9D%E9%98%B6"><span class="toc-number">3.8.</span> <span class="toc-text">CNN初阶</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">3.8.1.</span> <span class="toc-text">构建卷积层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-3"><span class="toc-number">3.8.1.1.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#padding"><span class="toc-number">3.8.1.2.</span> <span class="toc-text">padding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#stride"><span class="toc-number">3.8.1.3.</span> <span class="toc-text">stride</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Max-Pooling-Layer"><span class="toc-number">3.8.2.</span> <span class="toc-text">Max Pooling Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E4%B8%80%E4%BB%A3%E7%A0%81%E6%80%BB%E7%BB%93"><span class="toc-number">3.8.3.</span> <span class="toc-text">实例一代码总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%98%BE%E5%8D%A1%E8%BF%90%E7%AE%97"><span class="toc-number">3.8.4.</span> <span class="toc-text">显卡运算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7CNN"><span class="toc-number">3.9.</span> <span class="toc-text">高级CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GoogleNet"><span class="toc-number">3.9.1.</span> <span class="toc-text">GoogleNet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-4"><span class="toc-number">3.9.1.1.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet"><span class="toc-number">3.9.2.</span> <span class="toc-text">ResNet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MNIST-again"><span class="toc-number">3.9.2.1.</span> <span class="toc-text">MNIST again</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-5"><span class="toc-number">3.9.2.2.</span> <span class="toc-text">代码实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%ACRNN"><span class="toc-number">3.10.</span> <span class="toc-text">基本RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN%E5%B0%8F%E4%BB%BB%E5%8A%A1"><span class="toc-number">3.10.1.</span> <span class="toc-text">RNN小任务</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-6"><span class="toc-number">3.10.1.1.</span> <span class="toc-text">代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#RNNCell%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.10.1.1.1.</span> <span class="toc-text">RNNCell实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.10.1.2.</span> <span class="toc-text">改进模型</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#RNN%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.10.1.2.1.</span> <span class="toc-text">RNN实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Embedding"><span class="toc-number">3.10.2.</span> <span class="toc-text">Embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E5%90%8E%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.10.2.1.</span> <span class="toc-text">最后实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7RNN"><span class="toc-number">3.11.</span> <span class="toc-text">高级RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">3.11.1.</span> <span class="toc-text">例子</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-number">3.11.1.1.</span> <span class="toc-text">准备数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%BD%91%E7%BB%9C"><span class="toc-number">3.11.1.2.</span> <span class="toc-text">双向循环网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-7"><span class="toc-number">3.11.1.3.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%9A%E5%8F%A4%E8%AF%97%EF%BC%88%E4%BA%86%E8%A7%A3%EF%BC%89"><span class="toc-number">3.11.2.</span> <span class="toc-text">做古诗（了解）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7"><span class="toc-number">3.11.2.1.</span> <span class="toc-text">重要性采样</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B0%BE%E8%A8%80"><span class="toc-number">4.</span> <span class="toc-text">尾言</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://s2.loli.net/2022/04/15/YXoFfbxgWZIn1SM.png')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By jjyaoao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Livere' === 'Livere' || !true) {
  if (true) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'cFvw1zyA64uttBS5jbYY7pwH-gzGzoHsz',
      appKey: '14rOjAUNzRyJSQt983q6gdza',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Livere' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善,感谢" data-fontsize="15px" data-random="false" async="async"></script><script>(function(d, w, c) {
    w.ChatraID = 'zgYNWkNCSATLRhpg4';
    var s = d.createElement('script');
    w[c] = w[c] || function() {
        (w[c].q = w[c].q || []).push(arguments);
    };
    s.async = true;
    s.src = 'https://call.chatra.io/chatra.js';
    if (d.head) d.head.appendChild(s);
})(document, window, 'Chatra');

if (true) {
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      Chatra('openChat')
    });
  }
  chatBtnFn()
} else {
  if (true) {
    function chatBtnHide () {
      Chatra('hide')
    }
    function chatBtnShow () {
      Chatra('show')
    }
  }
}</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>